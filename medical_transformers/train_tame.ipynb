{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T22:47:44.622683Z",
     "start_time": "2018-08-09T22:47:43.825607Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntrougkas/.cache/pypoetry/virtualenvs/tame-cJ7vbEx--py3.8/lib/python3.8/site-packages/umap/plot.py:203: NumbaDeprecationWarning: The keyword argument 'nopython=False' was supplied. From Numba 0.59.0 the default is being changed to True and use of 'nopython=False' will raise a warning as the argument will have no effect. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit(nopython=False)\n"
     ]
    }
   ],
   "source": [
    "import classification as mt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'  # adapt plots for retina displays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model, dataset and brain area masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APTOS2019-TAME\t   misc\t\t    README.md\t      train_tame.ipynb\n",
      "classification.py  models_and_misc  requirements.txt  utils\n",
      "defaults\t   params.json\t    self_supervised   wandb\n",
      "docker_image\t   __pycache__\t    _torchshow\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T22:37:55.653436Z",
     "start_time": "2018-08-09T22:37:53.745827Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising Dataloaders . . .\n",
      "The default metric has been set to : \u001b[94mcohen_kappa\u001b[0m\n",
      "Initialising Model . . .\n",
      "Initialising Optimization methods . . \n",
      "LARS OPTIMIZER: \u001b[93m INACTIVE \u001b[0m\n",
      "Loading model from /home/ntrougkas/Documents/3D-TAME/medical_transformers/models_and_misc/CVAMD2021/checkpoints/APTOS2019_trans-pre_dino\n"
     ]
    }
   ],
   "source": [
    "net, train_loader, num_classes = mt.get_jupyter_items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T22:37:55.714671Z",
     "start_time": "2018-08-09T22:37:55.687817Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (backbone): VisionTransformer(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (blocks): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "    (fc): Identity()\n",
       "  )\n",
       "  (fc): Linear(in_features=384, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Important: Set model to eval before using any interpretation methods\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build TAME model (from scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tame.utilities.pl_module import TAMELIT\n",
    "\n",
    "version = \"TAME\"\n",
    "model = net.cpu()\n",
    "layers = [\n",
    "    \"backbone.blocks.9\",\n",
    "    \"backbone.blocks.10\",\n",
    "    \"backbone.blocks.11\",\n",
    "]\n",
    "epochs = 8\n",
    "\n",
    "pl_model = TAMELIT(\n",
    "    model_name=\"vit_b_16\",\n",
    "    model=model,\n",
    "    layers=layers,\n",
    "    attention_version=version,\n",
    "    train_method=\"new\",\n",
    "    lr=0.001,\n",
    "    epochs=epochs,\n",
    "    num_classes=num_classes,\n",
    "    eval_length=\"long\",\n",
    "    LeRF=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "\n",
    "wandb_logger = WandbLogger(project=\"APTOS2019-TAME\", name=\"random initialization\")\n",
    "trainer = pl.Trainer(\n",
    "    enable_checkpointing=True,\n",
    "    max_epochs=epochs,\n",
    "    logger=wandb_logger,\n",
    "    log_every_n_steps=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.set_float32_matmul_precision(\"medium\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "print(next(iter(train_loader))[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntrougkas/.cache/pypoetry/virtualenvs/tame-cJ7vbEx--py3.8/lib/python3.8/site-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmarios1861\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20231115_024801-6i01rwf8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marios1861/APTOS2019-TAME/runs/6i01rwf8' target=\"_blank\">random initialization</a></strong> to <a href='https://wandb.ai/marios1861/APTOS2019-TAME' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marios1861/APTOS2019-TAME' target=\"_blank\">https://wandb.ai/marios1861/APTOS2019-TAME</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marios1861/APTOS2019-TAME/runs/6i01rwf8' target=\"_blank\">https://wandb.ai/marios1861/APTOS2019-TAME/runs/6i01rwf8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\n",
      "  | Name           | Type               | Params\n",
      "------------------------------------------------------\n",
      "0 | generic        | Generic            | 22.1 M\n",
      "1 | train_accuracy | MulticlassAccuracy | 0     \n",
      "2 | val_accuracy   | MulticlassAccuracy | 0     \n",
      "------------------------------------------------------\n",
      "451 K     Trainable params\n",
      "21.7 M    Non-trainable params\n",
      "22.1 M    Total params\n",
      "88.477    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fcbde1465ec4309bf387617aa968057",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=8` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(pl_model, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate with AD and IC metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising Dataloaders . . .\n",
      "The default metric has been set to : \u001b[94mcohen_kappa\u001b[0m\n",
      "Initialising Model . . .\n",
      "Initialising Optimization methods . . \n",
      "LARS OPTIMIZER: \u001b[93m INACTIVE \u001b[0m\n",
      "Loading model from /home/ntrougkas/Documents/3D-TAME/medical_transformers/models_and_misc/CVAMD2021/checkpoints/APTOS2019_trans-pre_dino\n"
     ]
    }
   ],
   "source": [
    "_, test_loader, _ = mt.get_jupyter_items(train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntrougkas/.cache/pypoetry/virtualenvs/tame-cJ7vbEx--py3.8/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:492: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b917c6fcceb4fd9a06549e77059a618",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trainer.logger = CSVLogger(\"logs\", name=\"3D-TAME\")\n",
    "trainer.test(pl_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b69f1c9c159460aa1e304abdd4d4f39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=0.9866137828458106, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>training/Accuracy</td><td>█▁▁▂▃▂▁▂▂▂▁▃▂▃▂▁▁▂▂▂▂▂▃▂▂▂▂▂▂▂▂▃▂▂▂▁▂▃▂▂</td></tr><tr><td>training/CE</td><td>▁███████████████████████████████████████</td></tr><tr><td>training/Loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training/Mean</td><td>█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training/Var</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>7</td></tr><tr><td>trainer/global_step</td><td>319</td></tr><tr><td>training/Accuracy</td><td>0.17188</td></tr><tr><td>training/CE</td><td>2.67017</td></tr><tr><td>training/Loss</td><td>2.82357</td></tr><tr><td>training/Mean</td><td>0.1534</td></tr><tr><td>training/Var</td><td>0.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">random initialization</strong> at: <a href='https://wandb.ai/marios1861/APTOS2019-TAME/runs/6i01rwf8' target=\"_blank\">https://wandb.ai/marios1861/APTOS2019-TAME/runs/6i01rwf8</a><br/>Synced 5 W&B file(s), 2 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231115_024801-6i01rwf8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb_logger.finalize(\"success\")\n",
    "wandb_logger.experiment.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train deit-small TAME on ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ntrougkas/Documents/3D-TAME\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:timm.models.helpers:Loading pretrained weights from url (https://dl.fbaipublicfiles.com/deit/deit_small_patch16_224-cd65a155.pth)\n",
      "INFO:torch.distributed.nn.jit.instantiator:Created a temporary directory at /tmp/tmpo8zpjbme\n",
      "INFO:torch.distributed.nn.jit.instantiator:Writing /tmp/tmpo8zpjbme/_remote_module_non_scriptable.py\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import timm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from tame.utilities.pl_module import TAMELIT, LightnightDataset\n",
    "\n",
    "load_dotenv(\"/home/ntrougkas/Documents/3D-TAME/.env\")\n",
    "\n",
    "epochs = 8\n",
    "old_model = TAMELIT(\n",
    "    model=timm.create_model(\"deit_small_patch16_224\", pretrained=True),\n",
    "    model_name=\"vit_b_16\",\n",
    "    layers=[\"blocks.9\", \"blocks.10\", \"blocks.11\"],\n",
    "    attention_version=\"TAME\",\n",
    "    normalized_data=True,\n",
    "    lr=0.001,\n",
    "    epochs=epochs,\n",
    "    eval_length=\"short\",\n",
    ")\n",
    "\n",
    "imagenet_data = LightnightDataset(\n",
    "    dataset_path=Path(os.getenv(\"DATA\", \"./\")),\n",
    "    datalist_path=Path(os.getenv(\"LIST\", \"./\")),\n",
    "    model=\"vit_b_16\",\n",
    "    batch_size=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.set_float32_matmul_precision(\"medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntrougkas/.cache/pypoetry/virtualenvs/tame-cJ7vbEx--py3.8/lib/python3.8/site-packages/lightning/fabric/connector.py:565: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmarios1861\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20231115_132218-lw7hdyht</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marios1861/T-TAME-small/runs/lw7hdyht' target=\"_blank\">giddy-grass-9</a></strong> to <a href='https://wandb.ai/marios1861/T-TAME-small' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marios1861/T-TAME-small' target=\"_blank\">https://wandb.ai/marios1861/T-TAME-small</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marios1861/T-TAME-small/runs/lw7hdyht' target=\"_blank\">https://wandb.ai/marios1861/T-TAME-small/runs/lw7hdyht</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\n",
      "  | Name           | Type               | Params\n",
      "------------------------------------------------------\n",
      "0 | generic        | Generic            | 23.6 M\n",
      "1 | train_accuracy | MulticlassAccuracy | 0     \n",
      "2 | val_accuracy   | MulticlassAccuracy | 0     \n",
      "------------------------------------------------------\n",
      "1.6 M     Trainable params\n",
      "22.1 M    Non-trainable params\n",
      "23.6 M    Total params\n",
      "94.598    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eebbe9185f346b287fc2331f98b7e59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d9f37162b6a4f089600dcf2cbb85caa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntrougkas/.cache/pypoetry/virtualenvs/tame-cJ7vbEx--py3.8/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e024e2dd2254662a15338c169d0a6ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b5a29caa45c4173bcd339e5d190cddf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d0498d86243488da8503947f838e2af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82bc31be23cf49f4842b9673c7939fd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "655712265e284a5da8aa02bcaccd9aa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3a0220dea454a0f90b5466e4d80e86c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c50b23617cc46be91b44395595baa65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4642fadc282249b68120fdbbf7763129",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=8` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3c5da8425f04b24b6f3ebc878ebeca7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          AD 100%          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     21.06305503845215     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          AD 15%           </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     79.13764953613281     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          AD 50%           </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     33.58638381958008     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          IC 100%          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    14.050000190734863     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          IC 15%           </span>│<span style=\"color: #800080; text-decoration-color: #800080\">           4.25            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          IC 50%           </span>│<span style=\"color: #800080; text-decoration-color: #800080\">           17.25           </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m         AD 100%         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    21.06305503845215    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         AD 15%          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    79.13764953613281    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         AD 50%          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    33.58638381958008    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         IC 100%         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   14.050000190734863    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         IC 15%          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m          4.25           \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         IC 50%          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m          17.25          \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'AD 100%': 21.06305503845215,\n",
       "  'IC 100%': 14.050000190734863,\n",
       "  'AD 50%': 33.58638381958008,\n",
       "  'IC 50%': 17.25,\n",
       "  'AD 15%': 79.13764953613281,\n",
       "  'IC 15%': 4.25}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lightning.pytorch.loggers import WandbLogger\n",
    "import lightning.pytorch as pl\n",
    "\n",
    "wandb_logger = WandbLogger(project=\"T-TAME-small\")\n",
    "trainer = pl.Trainer(enable_checkpointing=True, max_epochs=epochs, logger=wandb_logger, precision=16)\n",
    "trainer.fit(old_model, imagenet_data)\n",
    "trainer.test(old_model, imagenet_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "250834010086436e8149bf54b372acd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.003 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AD 100%</td><td>▁</td></tr><tr><td>AD 15%</td><td>▁</td></tr><tr><td>AD 50%</td><td>▁</td></tr><tr><td>IC 100%</td><td>▁</td></tr><tr><td>IC 15%</td><td>▁</td></tr><tr><td>IC 50%</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>training/Accuracy</td><td>▃▅▆▃▃▅▅▅▁▅█▅█▇▄▅▇▇▇▅▇▆▇█▆▆▃▆▇▃▆▇▃▆▅▇▇▅█▇</td></tr><tr><td>training/CE</td><td>█▇▃▆▄▄▄▃▆▄▂▃▂▃▃▃▂▃▂▄▃▃▂▁▃▂▄▃▂▄▂▂▄▃▃▂▂▃▂▁</td></tr><tr><td>training/Loss</td><td>█▆▃▄▃▃▃▂▄▃▂▂▂▂▂▂▂▂▂▃▂▂▁▁▂▂▃▂▁▂▁▂▂▂▂▁▂▂▂▁</td></tr><tr><td>training/Mean</td><td>█▇▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training/Var</td><td>█▄▂▂▂▂▂▂▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation/accuracy</td><td>▁▁▁▁▁▁▁▁</td></tr><tr><td>validation/ce</td><td>▁▁▁▁▁▁▁▁</td></tr><tr><td>validation/loss</td><td>█▅▃▂▂▂▁▁</td></tr><tr><td>validation/mean</td><td>█▄▂▂▂▁▁▁</td></tr><tr><td>validation/var</td><td>█▅▄▃▃▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AD 100%</td><td>21.06306</td></tr><tr><td>AD 15%</td><td>79.13765</td></tr><tr><td>AD 50%</td><td>33.58638</td></tr><tr><td>IC 100%</td><td>14.05</td></tr><tr><td>IC 15%</td><td>4.25</td></tr><tr><td>IC 50%</td><td>17.25</td></tr><tr><td>epoch</td><td>8</td></tr><tr><td>trainer/global_step</td><td>160152</td></tr><tr><td>training/Accuracy</td><td>0.78125</td></tr><tr><td>training/CE</td><td>2.06598</td></tr><tr><td>training/Loss</td><td>2.74267</td></tr><tr><td>training/Mean</td><td>0.35326</td></tr><tr><td>training/Var</td><td>0.32343</td></tr><tr><td>validation/accuracy</td><td>1.0</td></tr><tr><td>validation/ce</td><td>0.67304</td></tr><tr><td>validation/loss</td><td>1.38637</td></tr><tr><td>validation/mean</td><td>0.36163</td></tr><tr><td>validation/var</td><td>0.35169</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">giddy-grass-9</strong> at: <a href='https://wandb.ai/marios1861/T-TAME-small/runs/lw7hdyht' target=\"_blank\">https://wandb.ai/marios1861/T-TAME-small/runs/lw7hdyht</a><br/>Synced 5 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231115_132218-lw7hdyht/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.alert(\n",
    "    title=\"Training finished\",\n",
    "    text=\"Training on deit-small finished, can finish medical transformers task\",\n",
    ")\n",
    "wandb_logger.finalize(\"success\")\n",
    "wandb_logger.experiment.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training complete, now we can finetune the trained TAME model on APTOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune TAME on APTOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APTOS2019-TAME\t   misc\t\t    README.md\t      train_tame.ipynb\n",
      "classification.py  models_and_misc  requirements.txt  utils\n",
      "defaults\t   params.json\t    self_supervised   wandb\n",
      "docker_image\t   __pycache__\t    _torchshow\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'medical_transformers'\n",
      "/home/ntrougkas/Documents/3D-TAME/medical_transformers\n"
     ]
    }
   ],
   "source": [
    "%cd medical_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntrougkas/.cache/pypoetry/virtualenvs/tame-cJ7vbEx--py3.8/lib/python3.8/site-packages/umap/plot.py:203: NumbaDeprecationWarning: The keyword argument 'nopython=False' was supplied. From Numba 0.59.0 the default is being changed to True and use of 'nopython=False' will raise a warning as the argument will have no effect. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit(nopython=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising Dataloaders . . .\n",
      "The default metric has been set to : \u001b[94mcohen_kappa\u001b[0m\n",
      "Initialising Model . . .\n",
      "Initialising Optimization methods . . \n",
      "LARS OPTIMIZER: \u001b[93m INACTIVE \u001b[0m\n",
      "Loading model from /home/ntrougkas/Documents/3D-TAME/medical_transformers/models_and_misc/CVAMD2021/checkpoints/APTOS2019_trans-pre_dino\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torch.distributed.nn.jit.instantiator:Created a temporary directory at /tmp/tmpah7kkekr\n",
      "INFO:torch.distributed.nn.jit.instantiator:Writing /tmp/tmpah7kkekr/_remote_module_non_scriptable.py\n"
     ]
    }
   ],
   "source": [
    "import classification as mt\n",
    "\n",
    "net, train_loader, num_classes = mt.get_jupyter_items()\n",
    "net.eval()\n",
    "from tame.utilities.pl_module import TAMELIT\n",
    "\n",
    "version = \"TAME\"\n",
    "model = net.cpu()\n",
    "layers = [\n",
    "    \"backbone.blocks.9\",\n",
    "    \"backbone.blocks.10\",\n",
    "    \"backbone.blocks.11\",\n",
    "]\n",
    "epochs = 8\n",
    "\n",
    "pl_model = TAMELIT(\n",
    "    model_name=\"vit_b_16\",\n",
    "    model=model,\n",
    "    layers=layers,\n",
    "    attention_version=version,\n",
    "    train_method=\"new\",\n",
    "    lr=0.0001,\n",
    "    epochs=epochs,\n",
    "    num_classes=num_classes,\n",
    "    eval_length=\"long\",\n",
    "    LeRF=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use new fusion module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:timm.models.helpers:Loading pretrained weights from url (https://dl.fbaipublicfiles.com/deit/deit_small_patch16_224-cd65a155.pth)\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "\n",
    "pretrained_model = TAMELIT.load_from_checkpoint(\n",
    "    \"/home/ntrougkas/Documents/3D-TAME/T-TAME-small/vkep09vs/checkpoints/epoch=7-step=320296.ckpt\",\n",
    "    model=timm.create_model(\"deit_small_patch16_224\", pretrained=True),\n",
    ")\n",
    "pretrained_model.generic.attn_mech[1].fuser = pl_model.generic.attn_mech[1].fuser\n",
    "pl_model.generic.attn_mech[1] = pretrained_model.generic.attn_mech[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ntrougkas/.cache/pypoetry/virtualenvs/tame-cJ7vbEx--py3.8/lib/python3.8/site-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmarios1861\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20240122_154246-8l5q7p5j</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marios1861/APTOS2019-TAME/runs/8l5q7p5j' target=\"_blank\">pretrained-new</a></strong> to <a href='https://wandb.ai/marios1861/APTOS2019-TAME' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marios1861/APTOS2019-TAME' target=\"_blank\">https://wandb.ai/marios1861/APTOS2019-TAME</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marios1861/APTOS2019-TAME/runs/8l5q7p5j' target=\"_blank\">https://wandb.ai/marios1861/APTOS2019-TAME/runs/8l5q7p5j</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\n",
      "  | Name           | Type               | Params\n",
      "------------------------------------------------------\n",
      "0 | generic        | Generic            | 22.1 M\n",
      "1 | train_accuracy | MulticlassAccuracy | 0     \n",
      "2 | val_accuracy   | MulticlassAccuracy | 0     \n",
      "------------------------------------------------------\n",
      "451 K     Trainable params\n",
      "21.7 M    Non-trainable params\n",
      "22.1 M    Total params\n",
      "88.477    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "597e4401ec3e4a6084eb4569037a9fbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntrougkas/.cache/pypoetry/virtualenvs/tame-cJ7vbEx--py3.8/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "`Trainer.fit` stopped: `max_epochs=8` reached.\n"
     ]
    }
   ],
   "source": [
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "import torch\n",
    "\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "wandb_logger = WandbLogger(project=\"APTOS2019-TAME\", name=\"pretrained-new\")\n",
    "trainer = pl.Trainer(\n",
    "    enable_checkpointing=True,\n",
    "    precision=\"16-mixed\",\n",
    "    gradient_clip_algorithm=\"norm\",\n",
    "    max_epochs=epochs,\n",
    "    logger=wandb_logger,\n",
    "    log_every_n_steps=1,\n",
    ")\n",
    "trainer.fit(pl_model, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for evaluating from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising Dataloaders . . .\n",
      "The default metric has been set to : \u001b[94mcohen_kappa\u001b[0m\n",
      "Initialising Model . . .\n",
      "Initialising Optimization methods . . \n",
      "LARS OPTIMIZER: \u001b[93m INACTIVE \u001b[0m\n",
      "Loading model from /home/ntrougkas/Documents/3D-TAME/medical_transformers/models_and_misc/CVAMD2021/checkpoints/APTOS2019_trans-pre_dino\n"
     ]
    }
   ],
   "source": [
    "\n",
    "net, test_loader, _ = mt.get_jupyter_items(train=False)\n",
    "net.eval()\n",
    "net.cpu()\n",
    "\n",
    "pl_model = TAMELIT.load_from_checkpoint(\n",
    "    # \"/home/ntrougkas/Documents/3D-TAME/medical_transformers/APTOS2019-TAME/6i01rwf8/checkpoints/epoch=7-step=320.ckpt\",\n",
    "    \"/home/ntrougkas/Documents/3D-TAME/medical_transformers/APTOS2019-TAME/d1el6nks/checkpoints/epoch=7-step=320.ckpt\",\n",
    "    model=net,\n",
    "    eval_length=\"short\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising Dataloaders . . .\n",
      "The default metric has been set to : \u001b[94mcohen_kappa\u001b[0m\n",
      "Initialising Model . . .\n",
      "Initialising Optimization methods . . \n",
      "LARS OPTIMIZER: \u001b[93m INACTIVE \u001b[0m\n",
      "Loading model from /home/ntrougkas/Documents/3D-TAME/medical_transformers/models_and_misc/CVAMD2021/checkpoints/APTOS2019_trans-pre_dino\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "578fdd295f3c4191bdc9a03ca6d881a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, test_loader, _ = mt.get_jupyter_items(train=False)\n",
    "trainer.test(pl_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b22d71d010aa42228c765dd8f2f5384d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>training/Accuracy</td><td>█▇▄▁▁▂▂▂▂▃▂▃▂▂▁▂▂▂▂▂▂▂▁▂▃▂▂▃▃▃▂▂▁▂▃▂▂▁▂▂</td></tr><tr><td>training/CE</td><td>▁▄▇█████████████████████████████████████</td></tr><tr><td>training/Loss</td><td>█▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training/Mean</td><td>█▆▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training/Var</td><td>█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>7</td></tr><tr><td>trainer/global_step</td><td>319</td></tr><tr><td>training/Accuracy</td><td>0.17188</td></tr><tr><td>training/CE</td><td>2.6724</td></tr><tr><td>training/Loss</td><td>2.8646</td></tr><tr><td>training/Mean</td><td>0.17552</td></tr><tr><td>training/Var</td><td>0.01668</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ablation_1</strong> at: <a href='https://wandb.ai/marios1861/APTOS2019-TAME/runs/3d7dj1q6' target=\"_blank\">https://wandb.ai/marios1861/APTOS2019-TAME/runs/3d7dj1q6</a><br/>Synced 5 W&B file(s), 2 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231116_015758-3d7dj1q6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb_logger.finalize(\"success\")\n",
    "wandb_logger.experiment.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do the same with new fuser module, retaining the older one and adapting it to new output classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntrougkas/.cache/pypoetry/virtualenvs/tame-cJ7vbEx--py3.8/lib/python3.8/site-packages/umap/plot.py:203: NumbaDeprecationWarning: The keyword argument 'nopython=False' was supplied. From Numba 0.59.0 the default is being changed to True and use of 'nopython=False' will raise a warning as the argument will have no effect. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit(nopython=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising Dataloaders . . .\n",
      "The default metric has been set to : \u001b[94mcohen_kappa\u001b[0m\n",
      "Initialising Model . . .\n",
      "Initialising Optimization methods . . \n",
      "LARS OPTIMIZER: \u001b[93m INACTIVE \u001b[0m\n",
      "Loading model from /home/ntrougkas/Documents/3D-TAME/medical_transformers/models_and_misc/CVAMD2021/checkpoints/APTOS2019_trans-pre_dino\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torch.distributed.nn.jit.instantiator:Created a temporary directory at /tmp/tmpyfbr9unm\n",
      "INFO:torch.distributed.nn.jit.instantiator:Writing /tmp/tmpyfbr9unm/_remote_module_non_scriptable.py\n"
     ]
    }
   ],
   "source": [
    "import classification as mt\n",
    "\n",
    "net, train_loader, num_classes = mt.get_jupyter_items()\n",
    "net.eval()\n",
    "from tame.utilities.pl_module import TAMELIT\n",
    "\n",
    "version = \"TAME\"\n",
    "model = net.cpu()\n",
    "layers = [\n",
    "    \"backbone.blocks.9\",\n",
    "    \"backbone.blocks.10\",\n",
    "    \"backbone.blocks.11\",\n",
    "]\n",
    "epochs = 8\n",
    "\n",
    "pl_model = TAMELIT(\n",
    "    model_name=\"vit_b_16\",\n",
    "    model=model,\n",
    "    layers=layers,\n",
    "    attention_version=version,\n",
    "    train_method=\"new\",\n",
    "    lr=0.0001,\n",
    "    epochs=epochs,\n",
    "    num_classes=num_classes,\n",
    "    eval_length=\"long\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapt existing fusion module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:timm.models.helpers:Loading pretrained weights from url (https://dl.fbaipublicfiles.com/deit/deit_small_patch16_224-cd65a155.pth)\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "from torch import nn\n",
    "\n",
    "pretrained_model = TAMELIT.load_from_checkpoint(\n",
    "    \"/home/ntrougkas/Documents/3D-TAME/T-TAME-small/vkep09vs/checkpoints/epoch=7-step=320296.ckpt\",\n",
    "    model=timm.create_model(\"deit_small_patch16_224\", pretrained=True),\n",
    ")\n",
    "pretrained_model.generic.attn_mech[1].fuser = nn.Sequential(\n",
    "    pretrained_model.generic.attn_mech[1].fuser,\n",
    "    nn.Conv2d(\n",
    "        in_channels=1000,\n",
    "        out_channels=num_classes,\n",
    "        kernel_size=1,\n",
    "        padding=0,\n",
    "        bias=True,\n",
    "    ),\n",
    ")\n",
    "pl_model.generic.attn_mech[1] = pretrained_model.generic.attn_mech[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ntrougkas/.cache/pypoetry/virtualenvs/tame-cJ7vbEx--py3.8/lib/python3.8/site-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmarios1861\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20231115_201242-rvuppps8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marios1861/APTOS2019-TAME/runs/rvuppps8' target=\"_blank\">pretrained_case3_short</a></strong> to <a href='https://wandb.ai/marios1861/APTOS2019-TAME' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marios1861/APTOS2019-TAME' target=\"_blank\">https://wandb.ai/marios1861/APTOS2019-TAME</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marios1861/APTOS2019-TAME/runs/rvuppps8' target=\"_blank\">https://wandb.ai/marios1861/APTOS2019-TAME/runs/rvuppps8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\n",
      "  | Name           | Type               | Params\n",
      "------------------------------------------------------\n",
      "0 | generic        | Generic            | 23.3 M\n",
      "1 | train_accuracy | MulticlassAccuracy | 0     \n",
      "2 | val_accuracy   | MulticlassAccuracy | 0     \n",
      "------------------------------------------------------\n",
      "1.6 M     Trainable params\n",
      "21.7 M    Non-trainable params\n",
      "23.3 M    Total params\n",
      "93.086    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a98d39b02bc84f96b3276b18a4fee63a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=8` reached.\n"
     ]
    }
   ],
   "source": [
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor\n",
    "\n",
    "import torch\n",
    "\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "wandb_logger = WandbLogger(project=\"APTOS2019-TAME\", name=\"pretrained_case3\")\n",
    "trainer = pl.Trainer(\n",
    "    enable_checkpointing=True,\n",
    "    max_epochs=epochs,\n",
    "    logger=wandb_logger,\n",
    "    log_every_n_steps=1,\n",
    "    callbacks=[\n",
    "        LearningRateMonitor(\n",
    "            logging_interval=\"step\", log_momentum=True, log_weight_decay=True\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "trainer.fit(pl_model, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising Dataloaders . . .\n",
      "The default metric has been set to : \u001b[94mcohen_kappa\u001b[0m\n",
      "Initialising Model . . .\n",
      "Initialising Optimization methods . . \n",
      "LARS OPTIMIZER: \u001b[93m INACTIVE \u001b[0m\n",
      "Loading model from /home/ntrougkas/Documents/3D-TAME/medical_transformers/models_and_misc/CVAMD2021/checkpoints/APTOS2019_trans-pre_dino\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/ntrougkas/.cache/pypoetry/virtualenvs/tame-cJ7vbEx--py3.8/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:492: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fff6723e5b849dcbb0356e3f6f14e0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, test_loader, _ = mt.get_jupyter_items(train=False)\n",
    "trainer.test(pl_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a7e31f0d1ed45739501c7f97ea3c2ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=0.9868551587301587, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>LR/pg1</td><td>▁▂▂▃▃▄▅▆▇▇███████▇▇▇▇▆▆▅▅▅▄▄▄▃▃▂▂▂▂▁▁▁▁▁</td></tr><tr><td>LR/pg1-momentum</td><td>██▇▇▆▅▄▃▂▂▁▁▁▁▁▁▁▂▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇▇█████</td></tr><tr><td>LR/pg1-weight_decay</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>LR/pg2</td><td>▁▂▂▃▃▄▅▆▇▇███████▇▇▇▇▆▆▅▅▅▄▄▄▃▃▂▂▂▂▁▁▁▁▁</td></tr><tr><td>LR/pg2-momentum</td><td>██▇▇▆▅▄▃▂▂▁▁▁▁▁▁▁▂▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇▇█████</td></tr><tr><td>LR/pg2-weight_decay</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>training/Accuracy</td><td>█▂▄▄▂▄▂▅▄▃▄█▃▅▆▅▆▃▅▆▄▇▃▃▄▃▁▅▃▃▅█▂▆▃▅▃▄▆▄</td></tr><tr><td>training/CE</td><td>▁██████▇███▇█▇██▇█▇██▇█████████▇█▇████▇█</td></tr><tr><td>training/Loss</td><td>█▂▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training/Mean</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training/Var</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>LR/pg1</td><td>0.0</td></tr><tr><td>LR/pg1-momentum</td><td>0.95</td></tr><tr><td>LR/pg1-weight_decay</td><td>0</td></tr><tr><td>LR/pg2</td><td>0.0</td></tr><tr><td>LR/pg2-momentum</td><td>0.95</td></tr><tr><td>LR/pg2-weight_decay</td><td>0.0005</td></tr><tr><td>epoch</td><td>7</td></tr><tr><td>trainer/global_step</td><td>319</td></tr><tr><td>training/Accuracy</td><td>0.15625</td></tr><tr><td>training/CE</td><td>2.67212</td></tr><tr><td>training/Loss</td><td>2.82554</td></tr><tr><td>training/Mean</td><td>0.15342</td></tr><tr><td>training/Var</td><td>0.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">pretrained_case3_short</strong> at: <a href='https://wandb.ai/marios1861/APTOS2019-TAME/runs/rvuppps8' target=\"_blank\">https://wandb.ai/marios1861/APTOS2019-TAME/runs/rvuppps8</a><br/>Synced 5 W&B file(s), 2 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231115_201242-rvuppps8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb_logger.finalize(\"success\")\n",
    "wandb_logger.experiment.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate AD and IC for competing methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First apply on gradcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising Dataloaders . . .\n",
      "The default metric has been set to : \u001b[94mcohen_kappa\u001b[0m\n",
      "Initialising Model . . .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising Optimization methods . . \n",
      "LARS OPTIMIZER: \u001b[93m INACTIVE \u001b[0m\n",
      "Loading model from /home/ntrougkas/Documents/3D-TAME/medical_transformers/models_and_misc/CVAMD2021/checkpoints/APTOS2019_trans-pre_dino\n"
     ]
    }
   ],
   "source": [
    "import classification as mt\n",
    "\n",
    "net, test_loader, _ = mt.get_jupyter_items(train=False)\n",
    "net.eval()\n",
    "from tame.utilities.comp_module import CompareModel\n",
    "import torch\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "\n",
    "model = net\n",
    "\n",
    "pl_model = CompareModel(\n",
    "    mdl_name=\"vit\",\n",
    "    name=\"gradcam\",\n",
    "    raw_model=model,\n",
    "    target_layers=[model.backbone.blocks[-1]],\n",
    "    eval_length=\"long\",\n",
    "    print_adic=True,\n",
    ")\n",
    "\n",
    "\n",
    "wandb_logger = WandbLogger(project=\"APTOS2019-TAME\", name=\"gradcam\")\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "trainer = pl.Trainer(logger=wandb_logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntrougkas/.cache/pypoetry/virtualenvs/tame-cJ7vbEx--py3.8/lib/python3.8/site-packages/lightning/pytorch/loggers/wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f060825c07ee454f86df0da7f5487f14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(pl_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25de5c1f1b4147b98299c37409b99458",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">gradcam</strong> at: <a href='https://wandb.ai/marios1861/APTOS2019-TAME/runs/0wrlwdm9' target=\"_blank\">https://wandb.ai/marios1861/APTOS2019-TAME/runs/0wrlwdm9</a><br/>Synced 5 W&B file(s), 2 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231115_020623-0wrlwdm9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb_logger.finalize(\"success\")\n",
    "wandb_logger.experiment.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do the same for the rest of the methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising Dataloaders . . .\n",
      "The default metric has been set to : \u001b[94mcohen_kappa\u001b[0m\n",
      "Initialising Model . . .\n",
      "Initialising Optimization methods . . \n",
      "LARS OPTIMIZER: \u001b[93m INACTIVE \u001b[0m\n",
      "Loading model from /home/ntrougkas/Documents/3D-TAME/medical_transformers/models_and_misc/CVAMD2021/checkpoints/APTOS2019_trans-pre_dino\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x7f5b5073fd90>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20240122_144836-c5p264p1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marios1861/APTOS2019-TAME/runs/c5p264p1' target=\"_blank\">gradcam++</a></strong> to <a href='https://wandb.ai/marios1861/APTOS2019-TAME' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marios1861/APTOS2019-TAME' target=\"_blank\">https://wandb.ai/marios1861/APTOS2019-TAME</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marios1861/APTOS2019-TAME/runs/c5p264p1' target=\"_blank\">https://wandb.ai/marios1861/APTOS2019-TAME/runs/c5p264p1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a3ab0bda84f4de3bd56c51cd14fa073",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4554d97805074f8f90545b6efe85ed54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.004 MB uploaded\\r'), FloatProgress(value=0.9454937938478144, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">gradcam++</strong> at: <a href='https://wandb.ai/marios1861/APTOS2019-TAME/runs/c5p264p1' target=\"_blank\">https://wandb.ai/marios1861/APTOS2019-TAME/runs/c5p264p1</a><br/>Synced 5 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240122_144836-c5p264p1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tame.utilities.comp_module import CompareModel\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "import classification as mt\n",
    "import torch\n",
    "\n",
    "methods = [\n",
    "    # \"scorecam\",\n",
    "    \"gradcam++\",\n",
    "]\n",
    "\n",
    "net, test_loader, _ = mt.get_jupyter_items(train=False)\n",
    "net.eval()\n",
    "model = net\n",
    "print(test_loader)\n",
    "for method_name in methods:\n",
    "    pl.utilities.memory.garbage_collection_cuda()\n",
    "    torch.cuda.empty_cache()\n",
    "    pl_model = CompareModel(\n",
    "        mdl_name=\"vit\",\n",
    "        name=method_name,\n",
    "        raw_model=model,\n",
    "        target_layers=[model.backbone.blocks[-1].norm1],\n",
    "        eval_length=\"short\",\n",
    "    )\n",
    "    if method_name == \"scorecam\":\n",
    "        pl_model.cam_model.batch_size = 4\n",
    "    wandb_logger = WandbLogger(project=\"APTOS2019-TAME\", name=method_name)\n",
    "    torch.set_float32_matmul_precision(\"medium\")\n",
    "    trainer = pl.Trainer(logger=wandb_logger)\n",
    "    trainer.test(pl_model, test_loader)\n",
    "    wandb_logger.finalize(\"success\")\n",
    "    wandb_logger.experiment.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate examples for TAME method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising Dataloaders . . .\n",
      "The default metric has been set to : \u001b[94mcohen_kappa\u001b[0m\n",
      "Initialising Model . . .\n",
      "Initialising Optimization methods . . \n",
      "LARS OPTIMIZER: \u001b[93m INACTIVE \u001b[0m\n",
      "Loading model from /home/ntrougkas/Documents/3D-TAME/medical_transformers/models_and_misc/CVAMD2021/checkpoints/APTOS2019_trans-pre_dino\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torchvision import models\n",
    "import classification as mt\n",
    "\n",
    "from tame.utilities.pl_module import TAMELIT\n",
    "net, test_loader, _ = mt.get_jupyter_items(train=False)\n",
    "net.cpu()\n",
    "model = TAMELIT.load_from_checkpoint(\n",
    "    # \"/home/ntrougkas/Documents/3D-TAME/medical_transformers/APTOS2019-TAME/6i01rwf8/checkpoints/epoch=7-step=320.ckpt\",\n",
    "    \"/home/ntrougkas/Documents/3D-TAME/medical_transformers/APTOS2019-TAME/d1el6nks/checkpoints/epoch=7-step=320.ckpt\",\n",
    "    model=net\n",
    ")\n",
    "# resnet50\n",
    "# model_name = \"resnet50\"\n",
    "# model = TAMELIT.load_from_checkpoint(\"logs/TAME_resnet50/version_5/checkpoints/epoch=3-step=160148.ckpt\", train_method=\"legacy\")\n",
    "\n",
    "# model: pl.LightningModule = torch.compile(model)  # type: ignore\n",
    "classes = [\n",
    "    \"No DR\",\n",
    "    \"Mild DR\",\n",
    "    \"Moderate DR\",\n",
    "    \"Severe DR\",\n",
    "    \"Prolific DR\",\n",
    "]\n",
    "images, labels = next(iter(test_loader))\n",
    "for id in [5]:\n",
    "    model.save_masked_image(\n",
    "        images[id],\n",
    "        id,\n",
    "        \"APTOS19-ViT\",\n",
    "        labels[id],\n",
    "        classes[labels[id]],\n",
    "        classes,\n",
    "        # select_mask=130\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate examples for other methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising Dataloaders . . .\n",
      "The default metric has been set to : \u001b[94mcohen_kappa\u001b[0m\n",
      "Initialising Model . . .\n",
      "Initialising Optimization methods . . \n",
      "LARS OPTIMIZER: \u001b[93m INACTIVE \u001b[0m\n",
      "Loading model from /home/ntrougkas/Documents/3D-TAME/medical_transformers/models_and_misc/CVAMD2021/checkpoints/APTOS2019_trans-pre_dino\n",
      "torch.Size([1, 1, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:00<00:00, 125.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 224, 224])\n",
      "torch.Size([1, 1, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torchvision import models\n",
    "import classification as mt\n",
    "\n",
    "from tame.utilities.comp_module import CompareModel\n",
    "\n",
    "net, test_loader, _ = mt.get_jupyter_items(train=False)\n",
    "net.eval()\n",
    "\n",
    "net.cpu()\n",
    "model = net\n",
    "\n",
    "methods = [\n",
    "    \"gradcam\",\n",
    "    \"scorecam\",\n",
    "    \"gradcam++\",\n",
    "]\n",
    "\n",
    "images, labels = next(iter(test_loader))\n",
    "for method_name in methods:\n",
    "    pl_model = CompareModel(\n",
    "        mdl_name=\"vit\",\n",
    "        name=method_name,\n",
    "        raw_model=model,\n",
    "        target_layers=[model.backbone.blocks[-1].norm1],\n",
    "        eval_length=\"long\",\n",
    "    )\n",
    "    mdl_truth = model(images[id].unsqueeze(0).cuda()).argmax().item()\n",
    "    classes = [\n",
    "        \"No DR\",\n",
    "        \"Mild DR\",\n",
    "        \"Moderate DR\",\n",
    "        \"Severe DR\",\n",
    "        \"Prolific DR\",\n",
    "    ]\n",
    "    for id in [5]:\n",
    "        pl_model.save_masked_image(\n",
    "            images[id],\n",
    "            id,\n",
    "            labels[id],\n",
    "            classes[labels[id]],\n",
    "            mdl_truth,\n",
    "            classes[mdl_truth]\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
