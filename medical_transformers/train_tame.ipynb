{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T22:47:44.622683Z",
     "start_time": "2018-08-09T22:47:43.825607Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntrougkas/Documents/3D-TAME/.venv/lib/python3.8/site-packages/umap/plot.py:203: NumbaDeprecationWarning: The keyword argument 'nopython=False' was supplied. From Numba 0.59.0 the default is being changed to True and use of 'nopython=False' will raise a warning as the argument will have no effect. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit(nopython=False)\n"
     ]
    }
   ],
   "source": [
    "import classification as mt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'  # adapt plots for retina displays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model, dataset and brain area masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APTOS2019-TAME\t   docker_image     params.json  requirements.txt  utils\n",
      "classification.py  misc\t\t    __pycache__  self_supervised   wandb\n",
      "defaults\t   models_and_misc  README.md\t train_tame.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T22:37:55.653436Z",
     "start_time": "2018-08-09T22:37:53.745827Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising Dataloaders . . .\n",
      "The default metric has been set to : \u001b[94mcohen_kappa\u001b[0m\n",
      "Initialising Model . . .\n",
      "Initialising Optimization methods . . \n",
      "LARS OPTIMIZER: \u001b[93m INACTIVE \u001b[0m\n",
      "Loading model from /home/ntrougkas/Documents/3D-TAME/medical_transformers/models_and_misc/CVAMD2021/checkpoints/APTOS2019_trans-pre_dino\n"
     ]
    }
   ],
   "source": [
    "net, train_loader, num_classes = mt.get_jupyter_items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T22:37:55.714671Z",
     "start_time": "2018-08-09T22:37:55.687817Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (backbone): VisionTransformer(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (blocks): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "    (fc): Identity()\n",
       "  )\n",
       "  (fc): Linear(in_features=384, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Important: Set model to eval before using any interpretation methods\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build TAME model (from scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torch.distributed.nn.jit.instantiator:Created a temporary directory at /tmp/tmpg085uts_\n",
      "INFO:torch.distributed.nn.jit.instantiator:Writing /tmp/tmpg085uts_/_remote_module_non_scriptable.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proxy(getattr_5) Proxy(getattr_6)\n",
      "Proxy(getattr_5) Proxy(getattr_6)\n",
      "Proxy(getattr_5) Proxy(getattr_6)\n",
      "Proxy(getattr_5) Proxy(getattr_6)\n"
     ]
    }
   ],
   "source": [
    "from tame.utilities.pl_module import TAMELIT\n",
    "\n",
    "version = \"TAME\"\n",
    "model = net.cpu()\n",
    "layers = [\n",
    "    \"backbone.blocks.9\",\n",
    "    \"backbone.blocks.10\",\n",
    "    \"backbone.blocks.11\",\n",
    "]\n",
    "epochs = 8\n",
    "\n",
    "pl_model = TAMELIT(\n",
    "    model_name=\"vit_b_16\",\n",
    "    model=model,\n",
    "    layers=layers,\n",
    "    attention_version=version,\n",
    "    train_method=\"new\",\n",
    "    lr=0.001,\n",
    "    epochs=epochs,\n",
    "    num_classes=num_classes,\n",
    "    eval_length=\"long\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "\n",
    "wandb_logger = WandbLogger(project=\"APTOS2019-TAME\", name=\"random-new\")\n",
    "trainer = pl.Trainer(\n",
    "    enable_checkpointing=True,\n",
    "    max_epochs=epochs,\n",
    "    logger=wandb_logger,\n",
    "    log_every_n_steps=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.set_float32_matmul_precision(\"medium\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "print(next(iter(train_loader))[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntrougkas/Documents/3D-TAME/.venv/lib/python3.8/site-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmarios1861\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20231114_205432-6e9g0tk1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marios1861/APTOS2019-TAME/runs/6e9g0tk1' target=\"_blank\">random-new</a></strong> to <a href='https://wandb.ai/marios1861/APTOS2019-TAME' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marios1861/APTOS2019-TAME' target=\"_blank\">https://wandb.ai/marios1861/APTOS2019-TAME</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marios1861/APTOS2019-TAME/runs/6e9g0tk1' target=\"_blank\">https://wandb.ai/marios1861/APTOS2019-TAME/runs/6e9g0tk1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\n",
      "  | Name           | Type               | Params\n",
      "------------------------------------------------------\n",
      "0 | generic        | Generic            | 22.1 M\n",
      "1 | train_accuracy | MulticlassAccuracy | 0     \n",
      "2 | val_accuracy   | MulticlassAccuracy | 0     \n",
      "------------------------------------------------------\n",
      "451 K     Trainable params\n",
      "21.7 M    Non-trainable params\n",
      "22.1 M    Total params\n",
      "88.477    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76c893789e1043689e01e4399512c384",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=8` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(pl_model, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate with AD and IC metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising Dataloaders . . .\n",
      "The default metric has been set to : \u001b[94mcohen_kappa\u001b[0m\n",
      "Initialising Model . . .\n",
      "Initialising Optimization methods . . \n",
      "LARS OPTIMIZER: \u001b[93m INACTIVE \u001b[0m\n",
      "Loading model from /home/ntrougkas/Documents/3D-TAME/medical_transformers/models_and_misc/CVAMD2021/checkpoints/APTOS2019_trans-pre_dino\n"
     ]
    }
   ],
   "source": [
    "_, test_loader, _ = mt.get_jupyter_items(train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/ntrougkas/Documents/3D-TAME/.venv/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:492: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d94b6f48662b4ce18fa9aeef446bafb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "shape mismatch: indexing tensors could not be broadcast together with shapes [64], [64, 224, 224]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/ntrougkas/Documents/3D-TAME/medical_transformers/train_tame.ipynb Cell 16\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bwork1/home/ntrougkas/Documents/3D-TAME/medical_transformers/train_tame.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# trainer.logger = CSVLogger(\"logs\", name=\"3D-TAME\")\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bwork1/home/ntrougkas/Documents/3D-TAME/medical_transformers/train_tame.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtest(pl_model, test_loader)\n",
      "File \u001b[0;32m~/Documents/3D-TAME/.venv/lib/python3.8/site-packages/lightning/pytorch/trainer/trainer.py:755\u001b[0m, in \u001b[0;36mTrainer.test\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[1;32m    753\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstatus \u001b[39m=\u001b[39m TrainerStatus\u001b[39m.\u001b[39mRUNNING\n\u001b[1;32m    754\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtesting \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 755\u001b[0m \u001b[39mreturn\u001b[39;00m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    756\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_test_impl, model, dataloaders, ckpt_path, verbose, datamodule\n\u001b[1;32m    757\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/3D-TAME/.venv/lib/python3.8/site-packages/lightning/pytorch/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     46\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/Documents/3D-TAME/.venv/lib/python3.8/site-packages/lightning/pytorch/trainer/trainer.py:795\u001b[0m, in \u001b[0;36mTrainer._test_impl\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[1;32m    791\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    792\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    793\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn, ckpt_path, model_provided\u001b[39m=\u001b[39mmodel_provided, model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    794\u001b[0m )\n\u001b[0;32m--> 795\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[1;32m    796\u001b[0m \u001b[39m# remove the tensors from the test results\u001b[39;00m\n\u001b[1;32m    797\u001b[0m results \u001b[39m=\u001b[39m convert_tensors_to_scalars(results)\n",
      "File \u001b[0;32m~/Documents/3D-TAME/.venv/lib/python3.8/site-packages/lightning/pytorch/trainer/trainer.py:990\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_signal_connector\u001b[39m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    987\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    988\u001b[0m \u001b[39m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    989\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 990\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m    992\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    993\u001b[0m \u001b[39m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    994\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    995\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/3D-TAME/.venv/lib/python3.8/site-packages/lightning/pytorch/trainer/trainer.py:1029\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module\u001b[39m.\u001b[39mzero_grad(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mzero_grad_kwargs)\n\u001b[1;32m   1028\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluating:\n\u001b[0;32m-> 1029\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluation_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m   1030\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[1;32m   1031\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict_loop\u001b[39m.\u001b[39mrun()\n",
      "File \u001b[0;32m~/Documents/3D-TAME/.venv/lib/python3.8/site-packages/lightning/pytorch/loops/utilities.py:181\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m     context_manager \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mno_grad\n\u001b[1;32m    180\u001b[0m \u001b[39mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 181\u001b[0m     \u001b[39mreturn\u001b[39;00m loop_run(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/3D-TAME/.venv/lib/python3.8/site-packages/lightning/pytorch/loops/evaluation_loop.py:134\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mis_last_batch \u001b[39m=\u001b[39m data_fetcher\u001b[39m.\u001b[39mdone\n\u001b[1;32m    133\u001b[0m     \u001b[39m# run step hooks\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n\u001b[1;32m    135\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m     \u001b[39m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[1;32m    137\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/3D-TAME/.venv/lib/python3.8/site-packages/lightning/pytorch/loops/evaluation_loop.py:391\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[1;32m    385\u001b[0m hook_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtest_step\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mtesting \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mvalidation_step\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    386\u001b[0m step_args \u001b[39m=\u001b[39m (\n\u001b[1;32m    387\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[1;32m    388\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m using_dataloader_iter\n\u001b[1;32m    389\u001b[0m     \u001b[39melse\u001b[39;00m (dataloader_iter,)\n\u001b[1;32m    390\u001b[0m )\n\u001b[0;32m--> 391\u001b[0m output \u001b[39m=\u001b[39m call\u001b[39m.\u001b[39;49m_call_strategy_hook(trainer, hook_name, \u001b[39m*\u001b[39;49mstep_args)\n\u001b[1;32m    393\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_processed()\n\u001b[1;32m    395\u001b[0m \u001b[39mif\u001b[39;00m using_dataloader_iter:\n\u001b[1;32m    396\u001b[0m     \u001b[39m# update the hook kwargs now that the step method might have consumed the iterator\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/3D-TAME/.venv/lib/python3.8/site-packages/lightning/pytorch/trainer/call.py:309\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00mtrainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 309\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    311\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    312\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/Documents/3D-TAME/.venv/lib/python3.8/site-packages/lightning/pytorch/strategies/strategy.py:416\u001b[0m, in \u001b[0;36mStrategy.test_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module:\n\u001b[1;32m    415\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_redirection(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module, \u001b[39m\"\u001b[39m\u001b[39mtest_step\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 416\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlightning_module\u001b[39m.\u001b[39;49mtest_step(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/3D-TAME/src/tame/utilities/pl_module.py:284\u001b[0m, in \u001b[0;36mTAMELIT.test_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    282\u001b[0m logits \u001b[39m=\u001b[39m logits\u001b[39m.\u001b[39msoftmax(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    283\u001b[0m chosen_logits, model_truth \u001b[39m=\u001b[39m logits\u001b[39m.\u001b[39mmax(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m--> 284\u001b[0m masks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgeneric\u001b[39m.\u001b[39;49mget_c(model_truth)\n\u001b[1;32m    285\u001b[0m masks \u001b[39m=\u001b[39m metrics\u001b[39m.\u001b[39mnormalizeMinMax(masks)\n\u001b[1;32m    286\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetric_AD_IC(images, chosen_logits, model_truth, masks)\n",
      "File \u001b[0;32m~/Documents/3D-TAME/src/tame/utilities/composite_models.py:135\u001b[0m, in \u001b[0;36mGeneric.get_c\u001b[0;34m(self, labels)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmasking \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdiagonal\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    134\u001b[0m     batches \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n\u001b[0;32m--> 135\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mc[torch\u001b[39m.\u001b[39;49marange(batches), labels, :, :]\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\n\u001b[1;32m    137\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmasking \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmax\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    138\u001b[0m     batched_select_max_masks \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mvmap(\n\u001b[1;32m    139\u001b[0m         Generic\u001b[39m.\u001b[39mselect_max_masks, in_dims\u001b[39m=\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    140\u001b[0m     )\n",
      "\u001b[0;31mIndexError\u001b[0m: shape mismatch: indexing tensors could not be broadcast together with shapes [64], [64, 224, 224]"
     ]
    }
   ],
   "source": [
    "# trainer.logger = CSVLogger(\"logs\", name=\"3D-TAME\")\n",
    "trainer.test(pl_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wandb_logger' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ntrougkas/Documents/3D-TAME/medical_transformers/train_tame.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bwork1/home/ntrougkas/Documents/3D-TAME/medical_transformers/train_tame.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m wandb_logger\u001b[39m.\u001b[39mfinalize(\u001b[39m\"\u001b[39m\u001b[39msuccess\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bwork1/home/ntrougkas/Documents/3D-TAME/medical_transformers/train_tame.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m wandb_logger\u001b[39m.\u001b[39mexperiment\u001b[39m.\u001b[39mfinish()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wandb_logger' is not defined"
     ]
    }
   ],
   "source": [
    "wandb_logger.finalize(\"success\")\n",
    "wandb_logger.experiment.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train deit-small TAME on ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ntrougkas/Documents/3D-TAME\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:timm.models.helpers:Loading pretrained weights from url (https://dl.fbaipublicfiles.com/deit/deit_small_patch16_224-cd65a155.pth)\n",
      "INFO:torch.distributed.nn.jit.instantiator:Created a temporary directory at /tmp/tmpkn_v02dm\n",
      "INFO:torch.distributed.nn.jit.instantiator:Writing /tmp/tmpkn_v02dm/_remote_module_non_scriptable.py\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import timm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from tame.utilities.pl_module import TAMELIT, LightnightDataset\n",
    "\n",
    "load_dotenv(\"/home/ntrougkas/Documents/3D-TAME/.env\")\n",
    "\n",
    "epochs = 8\n",
    "old_model = TAMELIT(\n",
    "    model=timm.create_model(\"deit_small_patch16_224\", pretrained=True),\n",
    "    model_name=\"vit_b_16\",\n",
    "    layers=[\"blocks.9\", \"blocks.10\", \"blocks.11\"],\n",
    "    attention_version=\"TAME\",\n",
    "    normalized_data=True,\n",
    "    lr=0.001,\n",
    "    epochs=epochs,\n",
    "    eval_length=\"short\",\n",
    ")\n",
    "\n",
    "imagenet_data = LightnightDataset(\n",
    "    dataset_path=Path(os.getenv(\"DATA\", \"./\")),\n",
    "    datalist_path=Path(os.getenv(\"LIST\", \"./\")),\n",
    "    model=\"vit_b_16\",\n",
    "    batch_size=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.set_float32_matmul_precision(\"medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmarios1861\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20231114_005203-vkep09vs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marios1861/T-TAME-small/runs/vkep09vs' target=\"_blank\">swift-shape-2</a></strong> to <a href='https://wandb.ai/marios1861/T-TAME-small' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marios1861/T-TAME-small' target=\"_blank\">https://wandb.ai/marios1861/T-TAME-small</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marios1861/T-TAME-small/runs/vkep09vs' target=\"_blank\">https://wandb.ai/marios1861/T-TAME-small/runs/vkep09vs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\n",
      "  | Name           | Type               | Params\n",
      "------------------------------------------------------\n",
      "0 | generic        | Generic            | 23.6 M\n",
      "1 | train_accuracy | MulticlassAccuracy | 0     \n",
      "2 | val_accuracy   | MulticlassAccuracy | 0     \n",
      "------------------------------------------------------\n",
      "1.6 M     Trainable params\n",
      "22.1 M    Non-trainable params\n",
      "23.6 M    Total params\n",
      "94.598    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69ef8a407bc8485b99aa68ed6d1f968b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8490283c64574d4fad3975fee3ac8179",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60c55cafcd2e4484bce2addbe15d2b16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12ee3224703c4b2aa0254134bd1802d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14afc929b0184f4a812be9b1fa21d5aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "175df9c06d814e7cb0ff73d201fca2b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f10834dccf964a5b995a5feb8318763d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96839e7c81d04d339eb309f8f831f015",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74ace4a5e6714a098c23ffe26c537fb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3914448e096e4c5792405f07338082db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=8` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bf891f7dbc347ffba52807221058206",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          AD 100%          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    12.129423141479492     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          AD 15%           </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     77.4417495727539      </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          AD 50%           </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     32.22406768798828     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          IC 100%          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    18.700000762939453     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          IC 15%           </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     4.700000286102295     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          IC 50%           </span>│<span style=\"color: #800080; text-decoration-color: #800080\">           22.0            </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m         AD 100%         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   12.129423141479492    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         AD 15%          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    77.4417495727539     \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         AD 50%          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    32.22406768798828    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         IC 100%         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   18.700000762939453    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         IC 15%          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    4.700000286102295    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         IC 50%          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m          22.0           \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'AD 100%': 12.129423141479492,\n",
       "  'IC 100%': 18.700000762939453,\n",
       "  'AD 50%': 32.22406768798828,\n",
       "  'IC 50%': 22.0,\n",
       "  'AD 15%': 77.4417495727539,\n",
       "  'IC 15%': 4.700000286102295}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lightning.pytorch.loggers import WandbLogger\n",
    "import lightning.pytorch as pl\n",
    "\n",
    "wandb_logger = WandbLogger(project=\"T-TAME-small\")\n",
    "trainer = pl.Trainer(enable_checkpointing=True, max_epochs=epochs, logger=wandb_logger)\n",
    "trainer.fit(old_model, imagenet_data)\n",
    "trainer.test(old_model, imagenet_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5c7d99566d24ceeb3eec0e56cb86036",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.003 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AD 100%</td><td>▁</td></tr><tr><td>AD 15%</td><td>▁</td></tr><tr><td>AD 50%</td><td>▁</td></tr><tr><td>IC 100%</td><td>▁</td></tr><tr><td>IC 15%</td><td>▁</td></tr><tr><td>IC 50%</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>training/Accuracy</td><td>▄▂▁▂▅▅▄▂▇▅▆▃▂▆▃▃▂█▅▅▇▆▅▅▄▆▅▂▅▅▅▃▂▅▅▅▅▇▅▅</td></tr><tr><td>training/CE</td><td>▆▇█▆▄▃▅▅▃▅▂▄▇▃▇▄▅▁▃▅▄▃▂▃▄▂▃▇▂▃▃▄▆▂▄▃▅▃▄▃</td></tr><tr><td>training/Loss</td><td>█▇▇▅▄▅▄▅▃▄▂▄▇▃▆▃▄▁▃▄▄▃▂▃▃▂▃▆▂▂▃▃▅▂▃▂▃▂▃▂</td></tr><tr><td>training/Mean</td><td>█▆▅▄▃▃▂▂▂▂▁▂▂▂▂▁▂▁▂▂▂▂▂▂▂▁▁▂▂▁▂▁▂▁▂▂▂▂▂▂</td></tr><tr><td>training/Var</td><td>█▅▃▂▃█▃▃▄▃▃▃▄▃▂▃▃▃▃▂▃▄▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁</td></tr><tr><td>validation/accuracy</td><td>▁▁▁▁▁▁▁▁</td></tr><tr><td>validation/ce</td><td>▁▁▁▁▁▁▁▁</td></tr><tr><td>validation/loss</td><td>█▇▅▅▃▃▁▁</td></tr><tr><td>validation/mean</td><td>█▃▁▁▁▂▂▂</td></tr><tr><td>validation/var</td><td>██▆▆▃▃▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AD 100%</td><td>12.12942</td></tr><tr><td>AD 15%</td><td>77.44175</td></tr><tr><td>AD 50%</td><td>32.22407</td></tr><tr><td>IC 100%</td><td>18.7</td></tr><tr><td>IC 15%</td><td>4.7</td></tr><tr><td>IC 50%</td><td>22.0</td></tr><tr><td>epoch</td><td>8</td></tr><tr><td>trainer/global_step</td><td>320296</td></tr><tr><td>training/Accuracy</td><td>0.9375</td></tr><tr><td>training/CE</td><td>1.30158</td></tr><tr><td>training/Loss</td><td>1.88393</td></tr><tr><td>training/Mean</td><td>0.32185</td></tr><tr><td>training/Var</td><td>0.26051</td></tr><tr><td>validation/accuracy</td><td>0.7925</td></tr><tr><td>validation/ce</td><td>1.52235</td></tr><tr><td>validation/loss</td><td>2.20222</td></tr><tr><td>validation/mean</td><td>0.33603</td></tr><tr><td>validation/var</td><td>0.34385</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">swift-shape-2</strong> at: <a href='https://wandb.ai/marios1861/T-TAME-small/runs/vkep09vs' target=\"_blank\">https://wandb.ai/marios1861/T-TAME-small/runs/vkep09vs</a><br/>Synced 5 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231114_005203-vkep09vs/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.alert(\n",
    "    title=\"Training finished\",\n",
    "    text=\"Training on deit-small finished, can finish medical transformers task\",\n",
    ")\n",
    "wandb_logger.finalize(\"success\")\n",
    "wandb_logger.experiment.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training complete, now we can finetune the trained TAME model on APTOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune TAME on APTOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APTOS2019-TAME\t   docker_image     params.json  requirements.txt  utils\n",
      "classification.py  misc\t\t    __pycache__  self_supervised   wandb\n",
      "defaults\t   models_and_misc  README.md\t train_tame.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ntrougkas/Documents/3D-TAME/medical_transformers\n"
     ]
    }
   ],
   "source": [
    "%cd medical_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising Dataloaders . . .\n",
      "The default metric has been set to : \u001b[94mcohen_kappa\u001b[0m\n",
      "Initialising Model . . .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torch.distributed.nn.jit.instantiator:Created a temporary directory at /tmp/tmpx6pzbnco\n",
      "INFO:torch.distributed.nn.jit.instantiator:Writing /tmp/tmpx6pzbnco/_remote_module_non_scriptable.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising Optimization methods . . \n",
      "LARS OPTIMIZER: \u001b[93m INACTIVE \u001b[0m\n",
      "Loading model from /home/ntrougkas/Documents/3D-TAME/medical_transformers/models_and_misc/CVAMD2021/checkpoints/APTOS2019_trans-pre_dino\n",
      "Proxy(getattr_5) Proxy(getattr_6)\n",
      "Proxy(getattr_5) Proxy(getattr_6)\n",
      "Proxy(getattr_5) Proxy(getattr_6)\n",
      "Proxy(getattr_5) Proxy(getattr_6)\n"
     ]
    }
   ],
   "source": [
    "import classification as mt\n",
    "\n",
    "net, train_loader, num_classes = mt.get_jupyter_items()\n",
    "net.eval()\n",
    "from tame.utilities.pl_module import TAMELIT\n",
    "\n",
    "version = \"TAME\"\n",
    "model = net.cpu()\n",
    "layers = [\n",
    "    \"backbone.blocks.9\",\n",
    "    \"backbone.blocks.10\",\n",
    "    \"backbone.blocks.11\",\n",
    "]\n",
    "epochs = 8\n",
    "\n",
    "pl_model = TAMELIT(\n",
    "    model_name=\"vit_b_16\",\n",
    "    model=model,\n",
    "    layers=layers,\n",
    "    attention_version=version,\n",
    "    train_method=\"new\",\n",
    "    lr=0.001,\n",
    "    epochs=epochs,\n",
    "    num_classes=num_classes,\n",
    "    eval_length=\"long\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use new fusion module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:timm.models.helpers:Loading pretrained weights from url (https://dl.fbaipublicfiles.com/deit/deit_small_patch16_224-cd65a155.pth)\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "\n",
    "pretrained_model = TAMELIT.load_from_checkpoint(\n",
    "    \"/home/ntrougkas/Documents/3D-TAME/T-TAME-small/vkep09vs/checkpoints/epoch=7-step=320296.ckpt\",\n",
    "    model=timm.create_model(\"deit_small_patch16_224\", pretrained=True),\n",
    ")\n",
    "pretrained_model.generic.attn_mech[1].fuser = pl_model.generic.attn_mech[1].fuser\n",
    "pl_model.generic.attn_mech[1] = pretrained_model.generic.attn_mech[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ntrougkas/Documents/3D-TAME/.venv/lib/python3.8/site-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmarios1861\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20231114_145031-utkdj3td</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marios1861/APTOS2019-TAME/runs/utkdj3td' target=\"_blank\">pretrained</a></strong> to <a href='https://wandb.ai/marios1861/APTOS2019-TAME' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marios1861/APTOS2019-TAME' target=\"_blank\">https://wandb.ai/marios1861/APTOS2019-TAME</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marios1861/APTOS2019-TAME/runs/utkdj3td' target=\"_blank\">https://wandb.ai/marios1861/APTOS2019-TAME/runs/utkdj3td</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\n",
      "  | Name           | Type               | Params\n",
      "------------------------------------------------------\n",
      "0 | generic        | Generic            | 22.1 M\n",
      "1 | train_accuracy | MulticlassAccuracy | 0     \n",
      "2 | val_accuracy   | MulticlassAccuracy | 0     \n",
      "------------------------------------------------------\n",
      "451 K     Trainable params\n",
      "21.7 M    Non-trainable params\n",
      "22.1 M    Total params\n",
      "88.477    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b409ad93fc4448af8873dcdf659e6bc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=8` reached.\n"
     ]
    }
   ],
   "source": [
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "import torch\n",
    "\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "wandb_logger = WandbLogger(project=\"APTOS2019-TAME\", name=\"pretrained\")\n",
    "trainer = pl.Trainer(\n",
    "    enable_checkpointing=True,\n",
    "    max_epochs=epochs,\n",
    "    logger=wandb_logger,\n",
    "    log_every_n_steps=1,\n",
    ")\n",
    "trainer.fit(pl_model, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising Dataloaders . . .\n",
      "The default metric has been set to : \u001b[94mcohen_kappa\u001b[0m\n",
      "Initialising Model . . .\n",
      "Initialising Optimization methods . . \n",
      "LARS OPTIMIZER: \u001b[93m INACTIVE \u001b[0m\n",
      "Loading model from /home/ntrougkas/Documents/3D-TAME/medical_transformers/models_and_misc/CVAMD2021/checkpoints/APTOS2019_trans-pre_dino\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/ntrougkas/Documents/3D-TAME/.venv/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:492: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49105024685a4327a447c284d0d30ee8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, test_loader, _ = mt.get_jupyter_items(train=False)\n",
    "trainer.test(pl_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43c047c2577a4616a9ef8957048f01a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=0.9865269461077845, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>training/Accuracy</td><td>█▆▅▁▃▃▃▄▄▂▅▄▃▂▃▃▃▄▄▂▃▄▂▃▃▃▂▃▂▃▃▃▃▃▄▃▃▃▃▃</td></tr><tr><td>training/CE</td><td>▁▆██▇▇█▇▇█▇▇▇████▇▇█▇▇█▇▇▇▇███▇▇▇█▇█▇▇▇▇</td></tr><tr><td>training/Loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training/Mean</td><td>█▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training/Var</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>7</td></tr><tr><td>trainer/global_step</td><td>319</td></tr><tr><td>training/Accuracy</td><td>0.46875</td></tr><tr><td>training/CE</td><td>2.23892</td></tr><tr><td>training/Loss</td><td>2.39247</td></tr><tr><td>training/Mean</td><td>0.15355</td></tr><tr><td>training/Var</td><td>0.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">pretrained</strong> at: <a href='https://wandb.ai/marios1861/APTOS2019-TAME/runs/utkdj3td' target=\"_blank\">https://wandb.ai/marios1861/APTOS2019-TAME/runs/utkdj3td</a><br/>Synced 5 W&B file(s), 2 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231114_145031-utkdj3td/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb_logger.finalize(\"success\")\n",
    "wandb_logger.experiment.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do the same with new fuser module, retaining the older one and adapting it to new output classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntrougkas/Documents/3D-TAME/.venv/lib/python3.8/site-packages/umap/plot.py:203: NumbaDeprecationWarning: The keyword argument 'nopython=False' was supplied. From Numba 0.59.0 the default is being changed to True and use of 'nopython=False' will raise a warning as the argument will have no effect. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit(nopython=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising Dataloaders . . .\n",
      "The default metric has been set to : \u001b[94mcohen_kappa\u001b[0m\n",
      "Initialising Model . . .\n",
      "Initialising Optimization methods . . \n",
      "LARS OPTIMIZER: \u001b[93m INACTIVE \u001b[0m\n",
      "Loading model from /home/ntrougkas/Documents/3D-TAME/medical_transformers/models_and_misc/CVAMD2021/checkpoints/APTOS2019_trans-pre_dino\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torch.distributed.nn.jit.instantiator:Created a temporary directory at /tmp/tmp7rkablpl\n",
      "INFO:torch.distributed.nn.jit.instantiator:Writing /tmp/tmp7rkablpl/_remote_module_non_scriptable.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proxy(getattr_5) Proxy(getattr_6)\n",
      "Proxy(getattr_5) Proxy(getattr_6)\n",
      "Proxy(getattr_5) Proxy(getattr_6)\n",
      "Proxy(getattr_5) Proxy(getattr_6)\n"
     ]
    }
   ],
   "source": [
    "import classification as mt\n",
    "\n",
    "net, train_loader, num_classes = mt.get_jupyter_items()\n",
    "net.eval()\n",
    "from tame.utilities.pl_module import TAMELIT\n",
    "\n",
    "version = \"TAME\"\n",
    "model = net.cpu()\n",
    "layers = [\n",
    "    \"backbone.blocks.9\",\n",
    "    \"backbone.blocks.10\",\n",
    "    \"backbone.blocks.11\",\n",
    "]\n",
    "epochs = 8\n",
    "\n",
    "pl_model = TAMELIT(\n",
    "    model_name=\"vit_b_16\",\n",
    "    model=model,\n",
    "    layers=layers,\n",
    "    attention_version=version,\n",
    "    train_method=\"new\",\n",
    "    lr=0.0001,\n",
    "    epochs=epochs,\n",
    "    num_classes=num_classes,\n",
    "    eval_length=\"long\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapt existing fusion module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:timm.models.helpers:Loading pretrained weights from url (https://dl.fbaipublicfiles.com/deit/deit_small_patch16_224-cd65a155.pth)\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "from torch import nn\n",
    "\n",
    "pretrained_model = TAMELIT.load_from_checkpoint(\n",
    "    \"/home/ntrougkas/Documents/3D-TAME/T-TAME-small/vkep09vs/checkpoints/epoch=7-step=320296.ckpt\",\n",
    "    model=timm.create_model(\"deit_small_patch16_224\", pretrained=True),\n",
    ")\n",
    "pretrained_model.generic.attn_mech[1].fuser = nn.Sequential(\n",
    "    pretrained_model.generic.attn_mech[1].fuser,\n",
    "    nn.Conv2d(\n",
    "        in_channels=1000,\n",
    "        out_channels=num_classes,\n",
    "        kernel_size=1,\n",
    "        padding=0,\n",
    "        bias=True,\n",
    "    ),\n",
    ")\n",
    "pl_model.generic.attn_mech[1] = pretrained_model.generic.attn_mech[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ntrougkas/Documents/3D-TAME/.venv/lib/python3.8/site-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmarios1861\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20231114_153004-tqlv3alh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marios1861/APTOS2019-TAME/runs/tqlv3alh' target=\"_blank\">pretrained</a></strong> to <a href='https://wandb.ai/marios1861/APTOS2019-TAME' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marios1861/APTOS2019-TAME' target=\"_blank\">https://wandb.ai/marios1861/APTOS2019-TAME</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marios1861/APTOS2019-TAME/runs/tqlv3alh' target=\"_blank\">https://wandb.ai/marios1861/APTOS2019-TAME/runs/tqlv3alh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\n",
      "  | Name           | Type               | Params\n",
      "------------------------------------------------------\n",
      "0 | generic        | Generic            | 23.3 M\n",
      "1 | train_accuracy | MulticlassAccuracy | 0     \n",
      "2 | val_accuracy   | MulticlassAccuracy | 0     \n",
      "------------------------------------------------------\n",
      "1.6 M     Trainable params\n",
      "21.7 M    Non-trainable params\n",
      "23.3 M    Total params\n",
      "93.086    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "732263801fb34838a4c54ea642417775",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=8` reached.\n"
     ]
    }
   ],
   "source": [
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor\n",
    "\n",
    "import torch\n",
    "\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "wandb_logger = WandbLogger(project=\"APTOS2019-TAME\", name=\"pretrained\")\n",
    "trainer = pl.Trainer(\n",
    "    enable_checkpointing=True,\n",
    "    max_epochs=epochs,\n",
    "    logger=wandb_logger,\n",
    "    log_every_n_steps=1,\n",
    "    callbacks=[\n",
    "        LearningRateMonitor(\n",
    "            logging_interval=\"step\", log_momentum=True, log_weight_decay=True\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "trainer.fit(pl_model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9214,  0.5485,  0.5358,  0.5041],\n",
       "        [-0.0627, -1.0408,  0.4921,  0.2686],\n",
       "        [ 0.6596, -0.0087, -0.2173, -0.6798],\n",
       "        [-0.4622, -1.5323, -0.9715, -0.5757]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(4, 4)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 0, 0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(a, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising Dataloaders . . .\n",
      "The default metric has been set to : \u001b[94mcohen_kappa\u001b[0m\n",
      "Initialising Model . . .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising Optimization methods . . \n",
      "LARS OPTIMIZER: \u001b[93m INACTIVE \u001b[0m\n",
      "Loading model from /home/ntrougkas/Documents/3D-TAME/medical_transformers/models_and_misc/CVAMD2021/checkpoints/APTOS2019_trans-pre_dino\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntrougkas/Documents/3D-TAME/.venv/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:492: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f93cfe12e924756b4f302b2644b1502",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, test_loader, _ = mt.get_jupyter_items(train=False)\n",
    "trainer.test(pl_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f6f451b6bff4ca097f5a368e9d03659",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=0.9867863375716779, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>LR/pg1</td><td>▁▂▂▃▃▄▅▆▇▇███████▇▇▇▇▆▆▅▅▅▄▄▄▃▃▂▂▂▂▁▁▁▁▁</td></tr><tr><td>LR/pg1-momentum</td><td>██▇▇▆▅▄▃▂▂▁▁▁▁▁▁▁▂▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇▇█████</td></tr><tr><td>LR/pg1-weight_decay</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>LR/pg2</td><td>▁▂▂▃▃▄▅▆▇▇███████▇▇▇▇▆▆▅▅▅▄▄▄▃▃▂▂▂▂▁▁▁▁▁</td></tr><tr><td>LR/pg2-momentum</td><td>██▇▇▆▅▄▃▂▂▁▁▁▁▁▁▁▂▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇▇█████</td></tr><tr><td>LR/pg2-weight_decay</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>training/Accuracy</td><td>▆█▄▄▅▅▆▃▄▅▅▃▃▆▄▄▃▃▄▁▆▁▃▅▅▅▄▆▅▅▂▃▆▅▃▅▅▄▃▇</td></tr><tr><td>training/CE</td><td>▁▆▇▇▆▇▆▇▇▆▆▇▆▅▇▆▇▇▅█▅▇▇▆▆▆▆▆▆▆█▇▆▇▇▆▅▆▇▅</td></tr><tr><td>training/Loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training/Mean</td><td>█▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training/Var</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>LR/pg1</td><td>0.0</td></tr><tr><td>LR/pg1-momentum</td><td>0.95</td></tr><tr><td>LR/pg1-weight_decay</td><td>0</td></tr><tr><td>LR/pg2</td><td>0.0</td></tr><tr><td>LR/pg2-momentum</td><td>0.95</td></tr><tr><td>LR/pg2-weight_decay</td><td>0.0005</td></tr><tr><td>epoch</td><td>7</td></tr><tr><td>trainer/global_step</td><td>319</td></tr><tr><td>training/Accuracy</td><td>0.85938</td></tr><tr><td>training/CE</td><td>1.48091</td></tr><tr><td>training/Loss</td><td>1.7922</td></tr><tr><td>training/Mean</td><td>0.23227</td></tr><tr><td>training/Var</td><td>0.07902</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">pretrained</strong> at: <a href='https://wandb.ai/marios1861/APTOS2019-TAME/runs/tqlv3alh' target=\"_blank\">https://wandb.ai/marios1861/APTOS2019-TAME/runs/tqlv3alh</a><br/>Synced 5 W&B file(s), 2 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231114_153004-tqlv3alh/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb_logger.finalize(\"success\")\n",
    "wandb_logger.experiment.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate AD and IC for competing methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First apply on gradcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntrougkas/Documents/3D-TAME/.venv/lib/python3.8/site-packages/umap/plot.py:203: NumbaDeprecationWarning: The keyword argument 'nopython=False' was supplied. From Numba 0.59.0 the default is being changed to True and use of 'nopython=False' will raise a warning as the argument will have no effect. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit(nopython=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising Dataloaders . . .\n",
      "The default metric has been set to : \u001b[94mcohen_kappa\u001b[0m\n",
      "Initialising Model . . .\n",
      "Initialising Optimization methods . . \n",
      "LARS OPTIMIZER: \u001b[93m INACTIVE \u001b[0m\n",
      "Loading model from /home/ntrougkas/Documents/3D-TAME/medical_transformers/models_and_misc/CVAMD2021/checkpoints/APTOS2019_trans-pre_dino\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torch.distributed.nn.jit.instantiator:Created a temporary directory at /tmp/tmpuu12s0r9\n",
      "INFO:torch.distributed.nn.jit.instantiator:Writing /tmp/tmpuu12s0r9/_remote_module_non_scriptable.py\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "import classification as mt\n",
    "\n",
    "net, test_loader, _ = mt.get_jupyter_items(train=False)\n",
    "net.eval()\n",
    "from tame.utilities.comp_module import CompareModel\n",
    "import torch\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "\n",
    "model = net\n",
    "\n",
    "pl_model = CompareModel(\n",
    "    name=\"gradcam\",\n",
    "    raw_model=model,\n",
    "    target_layers=[model.backbone.blocks[11]],\n",
    "    eval_length=\"long\",\n",
    ")\n",
    "\n",
    "\n",
    "wandb_logger = WandbLogger(project=\"APTOS2019-TAME\", name=\"gradcam\")\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "trainer = pl.Trainer(logger=wandb_logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmarios1861\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf007c399a304aba9f7745d5b8ea1d55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112082547818621, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20231114_235527-482i3mx3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marios1861/APTOS2019-TAME/runs/482i3mx3' target=\"_blank\">gradcam</a></strong> to <a href='https://wandb.ai/marios1861/APTOS2019-TAME' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marios1861/APTOS2019-TAME' target=\"_blank\">https://wandb.ai/marios1861/APTOS2019-TAME</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marios1861/APTOS2019-TAME/runs/482i3mx3' target=\"_blank\">https://wandb.ai/marios1861/APTOS2019-TAME/runs/482i3mx3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/ntrougkas/Documents/3D-TAME/.venv/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:492: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2e330135b814b4c83b3be8b873f047b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 197, 384]) torch.Size([1, 197, 384])\n",
      "1\n",
      "torch.Size([64, 197, 384]) torch.Size([1, 197, 384])\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 7.79 GiB total capacity; 6.94 GiB already allocated; 35.75 MiB free; 7.09 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/home/ntrougkas/Documents/3D-TAME/medical_transformers/train_tame.ipynb Cell 50\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bwork1/home/ntrougkas/Documents/3D-TAME/medical_transformers/train_tame.ipynb#Y100sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtest(pl_model, test_loader)\n",
      "File \u001b[0;32m~/Documents/3D-TAME/.venv/lib/python3.8/site-packages/lightning/pytorch/trainer/trainer.py:755\u001b[0m, in \u001b[0;36mTrainer.test\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[1;32m    753\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstatus \u001b[39m=\u001b[39m TrainerStatus\u001b[39m.\u001b[39mRUNNING\n\u001b[1;32m    754\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtesting \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 755\u001b[0m \u001b[39mreturn\u001b[39;00m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    756\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_test_impl, model, dataloaders, ckpt_path, verbose, datamodule\n\u001b[1;32m    757\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/3D-TAME/.venv/lib/python3.8/site-packages/lightning/pytorch/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     46\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/Documents/3D-TAME/.venv/lib/python3.8/site-packages/lightning/pytorch/trainer/trainer.py:795\u001b[0m, in \u001b[0;36mTrainer._test_impl\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[1;32m    791\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    792\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    793\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn, ckpt_path, model_provided\u001b[39m=\u001b[39mmodel_provided, model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    794\u001b[0m )\n\u001b[0;32m--> 795\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[1;32m    796\u001b[0m \u001b[39m# remove the tensors from the test results\u001b[39;00m\n\u001b[1;32m    797\u001b[0m results \u001b[39m=\u001b[39m convert_tensors_to_scalars(results)\n",
      "File \u001b[0;32m~/Documents/3D-TAME/.venv/lib/python3.8/site-packages/lightning/pytorch/trainer/trainer.py:990\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_signal_connector\u001b[39m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    987\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    988\u001b[0m \u001b[39m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    989\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 990\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m    992\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    993\u001b[0m \u001b[39m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    994\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    995\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/3D-TAME/.venv/lib/python3.8/site-packages/lightning/pytorch/trainer/trainer.py:1029\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module\u001b[39m.\u001b[39mzero_grad(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mzero_grad_kwargs)\n\u001b[1;32m   1028\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluating:\n\u001b[0;32m-> 1029\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluation_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m   1030\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[1;32m   1031\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict_loop\u001b[39m.\u001b[39mrun()\n",
      "File \u001b[0;32m~/Documents/3D-TAME/.venv/lib/python3.8/site-packages/lightning/pytorch/loops/utilities.py:181\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m     context_manager \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mno_grad\n\u001b[1;32m    180\u001b[0m \u001b[39mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 181\u001b[0m     \u001b[39mreturn\u001b[39;00m loop_run(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/3D-TAME/.venv/lib/python3.8/site-packages/lightning/pytorch/loops/evaluation_loop.py:134\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mis_last_batch \u001b[39m=\u001b[39m data_fetcher\u001b[39m.\u001b[39mdone\n\u001b[1;32m    133\u001b[0m     \u001b[39m# run step hooks\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n\u001b[1;32m    135\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m     \u001b[39m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[1;32m    137\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/3D-TAME/.venv/lib/python3.8/site-packages/lightning/pytorch/loops/evaluation_loop.py:391\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[1;32m    385\u001b[0m hook_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtest_step\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mtesting \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mvalidation_step\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    386\u001b[0m step_args \u001b[39m=\u001b[39m (\n\u001b[1;32m    387\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[1;32m    388\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m using_dataloader_iter\n\u001b[1;32m    389\u001b[0m     \u001b[39melse\u001b[39;00m (dataloader_iter,)\n\u001b[1;32m    390\u001b[0m )\n\u001b[0;32m--> 391\u001b[0m output \u001b[39m=\u001b[39m call\u001b[39m.\u001b[39;49m_call_strategy_hook(trainer, hook_name, \u001b[39m*\u001b[39;49mstep_args)\n\u001b[1;32m    393\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_processed()\n\u001b[1;32m    395\u001b[0m \u001b[39mif\u001b[39;00m using_dataloader_iter:\n\u001b[1;32m    396\u001b[0m     \u001b[39m# update the hook kwargs now that the step method might have consumed the iterator\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/3D-TAME/.venv/lib/python3.8/site-packages/lightning/pytorch/trainer/call.py:309\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00mtrainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 309\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    311\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    312\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/Documents/3D-TAME/.venv/lib/python3.8/site-packages/lightning/pytorch/strategies/strategy.py:416\u001b[0m, in \u001b[0;36mStrategy.test_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module:\n\u001b[1;32m    415\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_redirection(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module, \u001b[39m\"\u001b[39m\u001b[39mtest_step\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 416\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlightning_module\u001b[39m.\u001b[39;49mtest_step(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/3D-TAME/src/tame/utilities/comp_module.py:229\u001b[0m, in \u001b[0;36mCompareModel.test_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    227\u001b[0m logits \u001b[39m=\u001b[39m logits\u001b[39m.\u001b[39msoftmax(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    228\u001b[0m chosen_logits, model_truth \u001b[39m=\u001b[39m logits\u001b[39m.\u001b[39mmax(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m--> 229\u001b[0m masks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcam_model(input_tensor\u001b[39m=\u001b[39;49mimages)\n\u001b[1;32m    230\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcount_fp:\n\u001b[1;32m    231\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw_model\u001b[39m.\u001b[39mfp_count)\n",
      "File \u001b[0;32m~/Documents/3D-TAME/.venv/lib/python3.8/site-packages/pytorch_grad_cam/base_cam.py:188\u001b[0m, in \u001b[0;36mBaseCAM.__call__\u001b[0;34m(self, input_tensor, targets, aug_smooth, eigen_smooth)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[39mif\u001b[39;00m aug_smooth \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    185\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward_augmentation_smoothing(\n\u001b[1;32m    186\u001b[0m         input_tensor, targets, eigen_smooth)\n\u001b[0;32m--> 188\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(input_tensor,\n\u001b[1;32m    189\u001b[0m                     targets, eigen_smooth)\n",
      "File \u001b[0;32m~/Documents/3D-TAME/.venv/lib/python3.8/site-packages/pytorch_grad_cam/base_cam.py:74\u001b[0m, in \u001b[0;36mBaseCAM.forward\u001b[0;34m(self, input_tensor, targets, eigen_smooth)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_input_gradient:\n\u001b[1;32m     71\u001b[0m     input_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mVariable(input_tensor,\n\u001b[1;32m     72\u001b[0m                                            requires_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 74\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactivations_and_grads(input_tensor)\n\u001b[1;32m     75\u001b[0m \u001b[39mif\u001b[39;00m targets \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m     target_categories \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(outputs\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mnumpy(), axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/3D-TAME/.venv/lib/python3.8/site-packages/pytorch_grad_cam/activations_and_gradients.py:42\u001b[0m, in \u001b[0;36mActivationsAndGradients.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgradients \u001b[39m=\u001b[39m []\n\u001b[1;32m     41\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivations \u001b[39m=\u001b[39m []\n\u001b[0;32m---> 42\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(x)\n",
      "File \u001b[0;32m~/Documents/3D-TAME/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1538\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1535\u001b[0m     bw_hook \u001b[39m=\u001b[39m hooks\u001b[39m.\u001b[39mBackwardHook(\u001b[39mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1536\u001b[0m     args \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1538\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1539\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1540\u001b[0m     \u001b[39mfor\u001b[39;00m hook_id, hook \u001b[39min\u001b[39;00m (\n\u001b[1;32m   1541\u001b[0m         \u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mitems(),\n\u001b[1;32m   1542\u001b[0m         \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mitems(),\n\u001b[1;32m   1543\u001b[0m     ):\n",
      "File \u001b[0;32m~/Documents/3D-TAME/medical_transformers/defaults/models.py:60\u001b[0m, in \u001b[0;36mClassifier.forward\u001b[0;34m(self, x, return_embedding)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, return_embedding\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m     44\u001b[0m     \u001b[39mwith\u001b[39;00m autocast(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_mixed_precision):\n\u001b[1;32m     45\u001b[0m         \n\u001b[1;32m     46\u001b[0m         \u001b[39m# if isinstance(x, list) and hasattr(cnn_models, self.backbone_type):\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[39m#         start_idx = end_idx             \u001b[39;00m\n\u001b[1;32m     59\u001b[0m         \u001b[39m# else:\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m         x_emb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackbone(x)\n\u001b[1;32m     62\u001b[0m         x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(x_emb)\n\u001b[1;32m     64\u001b[0m         \u001b[39m# if return_embedding:\u001b[39;00m\n\u001b[1;32m     65\u001b[0m         \u001b[39m#     return x, x_emb        \u001b[39;00m\n\u001b[1;32m     66\u001b[0m         \u001b[39m# else:\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/3D-TAME/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/3D-TAME/medical_transformers/defaults/transformers.py:186\u001b[0m, in \u001b[0;36mVisionTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_memory_efficient:\n\u001b[1;32m    185\u001b[0m     \u001b[39mfor\u001b[39;00m start_idx, _x \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(x):\n\u001b[0;32m--> 186\u001b[0m         _out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_features(_x)\n\u001b[1;32m    187\u001b[0m         _out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(_out)\n\u001b[1;32m    188\u001b[0m         \u001b[39mif\u001b[39;00m start_idx \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/3D-TAME/medical_transformers/defaults/transformers.py:221\u001b[0m, in \u001b[0;36mVisionTransformer.forward_features\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    218\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_drop(x)\n\u001b[1;32m    220\u001b[0m \u001b[39mfor\u001b[39;00m blk \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks:\n\u001b[0;32m--> 221\u001b[0m     x \u001b[39m=\u001b[39m blk(x)\n\u001b[1;32m    222\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    223\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm(x)\n",
      "File \u001b[0;32m~/Documents/3D-TAME/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/3D-TAME/medical_transformers/defaults/transformers.py:108\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x, return_attention)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, return_attention\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 108\u001b[0m     y, attn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm1(x))\n\u001b[1;32m    109\u001b[0m     \u001b[39mif\u001b[39;00m return_attention:\n\u001b[1;32m    110\u001b[0m         \u001b[39mreturn\u001b[39;00m attn\n",
      "File \u001b[0;32m~/Documents/3D-TAME/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/3D-TAME/medical_transformers/defaults/transformers.py:85\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     82\u001b[0m qkv \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mqkv(x)\u001b[39m.\u001b[39mreshape(B, N, \u001b[39m3\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, C \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads)\u001b[39m.\u001b[39mpermute(\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m4\u001b[39m)\n\u001b[1;32m     83\u001b[0m q, k, v \u001b[39m=\u001b[39m qkv[\u001b[39m0\u001b[39m], qkv[\u001b[39m1\u001b[39m], qkv[\u001b[39m2\u001b[39m]\n\u001b[0;32m---> 85\u001b[0m attn \u001b[39m=\u001b[39m (q \u001b[39m@\u001b[39;49m k\u001b[39m.\u001b[39;49mtranspose(\u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)) \u001b[39m*\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale\n\u001b[1;32m     86\u001b[0m attn \u001b[39m=\u001b[39m attn\u001b[39m.\u001b[39msoftmax(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     87\u001b[0m attn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattn_drop(attn)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 7.79 GiB total capacity; 6.94 GiB already allocated; 35.75 MiB free; 7.09 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "trainer.test(pl_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a5242527bf14d869bb8600436e1d77e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=0.9503891050583657, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">gradcam</strong> at: <a href='https://wandb.ai/marios1861/3D-TAME/runs/o4z88d94' target=\"_blank\">https://wandb.ai/marios1861/3D-TAME/runs/o4z88d94</a><br/>Synced 5 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231112_213614-o4z88d94/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb_logger.finalize(\"success\")\n",
    "wandb_logger.experiment.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do the same for the rest of the methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torch.distributed.nn.jit.instantiator:Created a temporary directory at /tmp/tmphild9ozc\n",
      "INFO:torch.distributed.nn.jit.instantiator:Writing /tmp/tmphild9ozc/_remote_module_non_scriptable.py\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmarios1861\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20231113_032104-zhst3trv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marios1861/3D-TAME/runs/zhst3trv' target=\"_blank\">gradcam++</a></strong> to <a href='https://wandb.ai/marios1861/3D-TAME' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marios1861/3D-TAME' target=\"_blank\">https://wandb.ai/marios1861/3D-TAME</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marios1861/3D-TAME/runs/zhst3trv' target=\"_blank\">https://wandb.ai/marios1861/3D-TAME/runs/zhst3trv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "052dc10163f442d7a673c138948b22a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "645251a916ca4ecd8779d93dbe036064",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=0.9508396203455829, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">gradcam++</strong> at: <a href='https://wandb.ai/marios1861/3D-TAME/runs/zhst3trv' target=\"_blank\">https://wandb.ai/marios1861/3D-TAME/runs/zhst3trv</a><br/>Synced 5 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231113_032104-zhst3trv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tame.utilities.comp_module import CompareModel\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "\n",
    "methods = [\n",
    "    \"scorecam\",\n",
    "    \"gradcam++\",\n",
    "    \"ablationcam\",\n",
    "]\n",
    "\n",
    "net, test_loader, _ = mt.get_jupyter_items(train=False)\n",
    "net.eval()\n",
    "model = net\n",
    "\n",
    "for method_name in methods:\n",
    "    pl.utilities.memory.garbage_collection_cuda()\n",
    "    torch.cuda.empty_cache()\n",
    "    pl_model = CompareModel(\n",
    "        mdl_name=\"vit\",\n",
    "        name=\"gradcam\",\n",
    "        raw_model=model,\n",
    "        target_layers=[model.backbone.blocks[11]],\n",
    "        eval_length=\"long\",\n",
    "    )\n",
    "    wandb_logger = WandbLogger(project=\"APTOS2019-TAME\", name=method_name)\n",
    "    torch.set_float32_matmul_precision(\"medium\")\n",
    "    trainer = pl.Trainer(logger=wandb_logger)\n",
    "    trainer.test(pl_model, test_loader)\n",
    "    wandb_logger.finalize(\"success\")\n",
    "    wandb_logger.experiment.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T01:35:14.988112Z",
     "start_time": "2018-08-07T01:33:38.233892Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set up a binary mask for each brain area based on the AAL atlas.\n",
    "# This is required for brain area occlusion and to determine the relevance per area.\n",
    "# The AAL atlas can for example be retrieved from here: https://github.com/neurolabusc/MRIcron/blob/master/templates/aal.nii.gz\n",
    "brain_map = utils.load_nifti(\"data/aal.nii.gz\")\n",
    "brain_areas = np.unique(brain_map)[1:]  # omit background\n",
    "\n",
    "area_masks = []\n",
    "for area in tqdm_notebook(brain_areas):\n",
    "    area_mask = np.zeros_like(brain_map)\n",
    "    area_mask[brain_map == area] = 1\n",
    "    area_mask = utils.resize_image(\n",
    "        area_mask, test_dataset.image_shape(), interpolation=0\n",
    "    )\n",
    "    area_masks.append(area_mask)\n",
    "\n",
    "area_names = [\n",
    "    \"Precentral_L\",\n",
    "    \"Precentral_R\",\n",
    "    \"Frontal_Sup_L\",\n",
    "    \"Frontal_Sup_R\",\n",
    "    \"Frontal_Sup_Orb_L\",\n",
    "    \"Frontal_Sup_Orb_R\",\n",
    "    \"Frontal_Mid_L\",\n",
    "    \"Frontal_Mid_R\",\n",
    "    \"Frontal_Mid_Orb_L\",\n",
    "    \"Frontal_Mid_Orb_R\",\n",
    "    \"Frontal_Inf_Oper_L\",\n",
    "    \"Frontal_Inf_Oper_R\",\n",
    "    \"Frontal_Inf_Tri_L\",\n",
    "    \"Frontal_Inf_Tri_R\",\n",
    "    \"Frontal_Inf_Orb_L\",\n",
    "    \"Frontal_Inf_Orb_R\",\n",
    "    \"Rolandic_Oper_L\",\n",
    "    \"Rolandic_Oper_R\",\n",
    "    \"Supp_Motor_Area_L\",\n",
    "    \"Supp_Motor_Area_R\",\n",
    "    \"Olfactory_L\",\n",
    "    \"Olfactory_R\",\n",
    "    \"Frontal_Sup_Medial_L\",\n",
    "    \"Frontal_Sup_Medial_R\",\n",
    "    \"Frontal_Med_Orb_L\",\n",
    "    \"Frontal_Med_Orb_R\",\n",
    "    \"Rectus_L\",\n",
    "    \"Rectus_R\",\n",
    "    \"Insula_L\",\n",
    "    \"Insula_R\",\n",
    "    \"Cingulum_Ant_L\",\n",
    "    \"Cingulum_Ant_R\",\n",
    "    \"Cingulum_Mid_L\",\n",
    "    \"Cingulum_Mid_R\",\n",
    "    \"Cingulum_Post_L\",\n",
    "    \"Cingulum_Post_R\",\n",
    "    \"Hippocampus_L\",\n",
    "    \"Hippocampus_R\",\n",
    "    \"ParaHippocampal_L\",\n",
    "    \"ParaHippocampal_R\",\n",
    "    \"Amygdala_L\",\n",
    "    \"Amygdala_R\",\n",
    "    \"Calcarine_L\",\n",
    "    \"Calcarine_R\",\n",
    "    \"Cuneus_L\",\n",
    "    \"Cuneus_R\",\n",
    "    \"Lingual_L\",\n",
    "    \"Lingual_R\",\n",
    "    \"Occipital_Sup_L\",\n",
    "    \"Occipital_Sup_R\",\n",
    "    \"Occipital_Mid_L\",\n",
    "    \"Occipital_Mid_R\",\n",
    "    \"Occipital_Inf_L\",\n",
    "    \"Occipital_Inf_R\",\n",
    "    \"Fusiform_L\",\n",
    "    \"Fusiform_R\",\n",
    "    \"Postcentral_L\",\n",
    "    \"Postcentral_R\",\n",
    "    \"Parietal_Sup_L\",\n",
    "    \"Parietal_Sup_R\",\n",
    "    \"Parietal_Inf_L\",\n",
    "    \"Parietal_Inf_R\",\n",
    "    \"SupraMarginal_L\",\n",
    "    \"SupraMarginal_R\",\n",
    "    \"Angular_L\",\n",
    "    \"Angular_R\",\n",
    "    \"Precuneus_L\",\n",
    "    \"Precuneus_R\",\n",
    "    \"Paracentral_Lobule_L\",\n",
    "    \"Paracentral_Lobule_R\",\n",
    "    \"Caudate_L\",\n",
    "    \"Caudate_R\",\n",
    "    \"Putamen_L\",\n",
    "    \"Putamen_R\",\n",
    "    \"Pallidum_L\",\n",
    "    \"Pallidum_R\",\n",
    "    \"Thalamus_L\",\n",
    "    \"Thalamus_R\",\n",
    "    \"Heschl_L\",\n",
    "    \"Heschl_R\",\n",
    "    \"Temporal_Sup_L\",\n",
    "    \"Temporal_Sup_R\",\n",
    "    \"Temporal_Pole_Sup_L\",\n",
    "    \"Temporal_Pole_Sup_R\",\n",
    "    \"Temporal_Mid_L\",\n",
    "    \"Temporal_Mid_R\",\n",
    "    \"Temporal_Pole_Mid_L\",\n",
    "    \"Temporal_Pole_Mid_R\",\n",
    "    \"Temporal_Inf_L\",\n",
    "    \"Temporal_Inf_R\",\n",
    "    \"Cerebelum_Crus1_L\",\n",
    "    \"Cerebelum_Crus1_R\",\n",
    "    \"Cerebelum_Crus2_L\",\n",
    "    \"Cerebelum_Crus2_R\",\n",
    "    \"Cerebelum_3_L\",\n",
    "    \"Cerebelum_3_R\",\n",
    "    \"Cerebelum_4_5_L\",\n",
    "    \"Cerebelum_4_5_R\",\n",
    "    \"Cerebelum_6_L\",\n",
    "    \"Cerebelum_6_R\",\n",
    "    \"Cerebelum_7b_L\",\n",
    "    \"Cerebelum_7b_R\",\n",
    "    \"Cerebelum_8_L\",\n",
    "    \"Cerebelum_8_R\",\n",
    "    \"Cerebelum_9_L\",\n",
    "    \"Cerebelum_9_R\",\n",
    "    \"Cerebelum_10_L\",\n",
    "    \"Cerebelum_10_R\",\n",
    "    \"Vermis_1_2\",\n",
    "    \"Vermis_3\",\n",
    "    \"Vermis_4_5\",\n",
    "    \"Vermis_6\",\n",
    "    \"Vermis_7\",\n",
    "    \"Vermis_8\",\n",
    "    \"Vermis_9\",\n",
    "    \"Vermis_10\",\n",
    "]\n",
    "\n",
    "# Merge left and right areas.\n",
    "merged_area_names = [name[:-2] for name in area_names[:108:2]] + area_names[108:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T01:35:21.349969Z",
     "start_time": "2018-08-07T01:35:14.989425Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_relevance_per_area(relevance_map, normalize=True):\n",
    "    relevances = np.zeros(len(area_masks))\n",
    "    for i, area_mask in enumerate(area_masks):\n",
    "        relevances[i] = np.sum(relevance_map * area_mask)\n",
    "    if normalize:\n",
    "        relevances /= relevances.sum()  # make all areas sum to 1\n",
    "\n",
    "    # Merge left and right areas.\n",
    "    merged_relevances = np.concatenate(\n",
    "        [relevances[:108].reshape(-1, 2).sum(1), relevances[108:]]\n",
    "    )\n",
    "\n",
    "    return sorted(\n",
    "        zip(merged_area_names, merged_relevances), key=lambda x: x[1], reverse=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relevance heatmaps for single images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T16:47:32.832908Z",
     "start_time": "2018-03-29T16:47:32.737673Z"
    },
    "collapsed": true
   },
   "source": [
    "## Raw image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T01:41:04.033336Z",
     "start_time": "2018-08-07T01:40:55.978520Z"
    }
   },
   "outputs": [],
   "source": [
    "which = 1\n",
    "\n",
    "image_tensor = test_dataset[which][0]\n",
    "raw_image = test_dataset.get_raw_image(which)\n",
    "\n",
    "utils.plot_slices(raw_image, num_slices=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T01:41:08.600496Z",
     "start_time": "2018-08-07T01:41:04.035220Z"
    }
   },
   "outputs": [],
   "source": [
    "relevance_map_backprop = interpretation.sensitivity_analysis(\n",
    "    net, image_tensor, cuda=True, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T01:41:11.655697Z",
     "start_time": "2018-08-07T01:41:08.602421Z"
    }
   },
   "outputs": [],
   "source": [
    "utils.plot_slices(\n",
    "    raw_image,\n",
    "    overlay=relevance_map_backprop[0],\n",
    "    overlay_vmax=np.percentile(relevance_map_backprop, 99.9),\n",
    "    overlay_cmap=utils.alpha_to_red_cmap,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T01:41:17.828447Z",
     "start_time": "2018-08-07T01:41:15.255941Z"
    }
   },
   "outputs": [],
   "source": [
    "get_relevance_per_area(relevance_map_backprop[0])[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guided Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T01:41:23.770149Z",
     "start_time": "2018-08-07T01:41:23.391138Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "relevance_map_guided = interpretation.guided_backprop(\n",
    "    net, image_tensor, cuda=True, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T01:41:32.019625Z",
     "start_time": "2018-08-07T01:41:29.218456Z"
    }
   },
   "outputs": [],
   "source": [
    "utils.plot_slices(\n",
    "    raw_image,\n",
    "    overlay=relevance_map_guided[0],\n",
    "    overlay_vmax=np.percentile(relevance_map_guided, 99.9),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T01:41:59.391097Z",
     "start_time": "2018-08-07T01:41:56.158676Z"
    }
   },
   "outputs": [],
   "source": [
    "get_relevance_per_area(relevance_map_guided[0])[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Occlusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T01:44:52.903618Z",
     "start_time": "2018-08-07T01:44:33.996801Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "relevance_map_occlusion = interpretation.occlusion(\n",
    "    net, image_tensor, size=40, stride=25, cuda=True, resize=True, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T01:44:55.599559Z",
     "start_time": "2018-08-07T01:44:52.905150Z"
    }
   },
   "outputs": [],
   "source": [
    "utils.plot_slices(\n",
    "    raw_image, overlay=relevance_map_occlusion, overlay_cmap=utils.alpha_to_red_cmap\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T01:42:43.680216Z",
     "start_time": "2018-08-07T01:42:39.516956Z"
    }
   },
   "outputs": [],
   "source": [
    "utils.plot_slices(\n",
    "    raw_image, overlay=relevance_map_occlusion, overlay_cmap=utils.alpha_to_red_cmap\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T01:43:22.952874Z",
     "start_time": "2018-08-07T01:43:16.589894Z"
    }
   },
   "outputs": [],
   "source": [
    "get_relevance_per_area(relevance_map_occlusion)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T01:43:23.087937Z",
     "start_time": "2018-08-07T01:43:22.954251Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Plot occlusion patch on image.\n",
    "occluded_image = raw_image[:, 114, :].copy()\n",
    "size = 40\n",
    "occluded_image[50 : 50 + size, 70 : 70 + size] = 0\n",
    "plt.imshow(occluded_image, cmap=\"gray\")\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Area Occlusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T01:43:28.272472Z",
     "start_time": "2018-08-07T01:43:23.089391Z"
    }
   },
   "outputs": [],
   "source": [
    "relevance_map_area_occlusion = interpretation.area_occlusion(\n",
    "    net, image_tensor, area_masks, cuda=True, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T01:43:31.052962Z",
     "start_time": "2018-08-07T01:43:28.273911Z"
    }
   },
   "outputs": [],
   "source": [
    "utils.plot_slices(\n",
    "    raw_image,\n",
    "    overlay=relevance_map_area_occlusion,\n",
    "    overlay_cmap=utils.alpha_to_red_cmap,\n",
    ")  # , overlay_vmin=0, overlay_vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T01:43:34.245910Z",
     "start_time": "2018-08-07T01:43:32.365503Z"
    }
   },
   "outputs": [],
   "source": [
    "get_relevance_per_area(relevance_map_area_occlusion)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grad-CAM\n",
    "Note: Grad-CAM doesn't work on this data. It's only included here for completeness, but I couldn't figure out if there's a bug in the implementation or if the method is not applicable to this kind of data and/or model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-04T14:40:58.960897Z",
     "start_time": "2018-08-04T14:40:58.392086Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Something is apparently wrong with gradcam, try to fix it. Could also simply be because feature maps of last layer are just 4x5x4, so very small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-04T14:40:59.266286Z",
     "start_time": "2018-08-04T14:40:59.043921Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "relevance_map_grad_cam = interpretation.grad_cam(\n",
    "    net, image_tensor, cuda=True, resize=True, interpolation=0, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-04T14:41:02.208918Z",
     "start_time": "2018-08-04T14:40:59.342875Z"
    }
   },
   "outputs": [],
   "source": [
    "utils.plot_slices(\n",
    "    raw_image, overlay=relevance_map_grad_cam * test_dataset.std + test_dataset.mean\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-04T14:41:04.903289Z",
     "start_time": "2018-08-04T14:41:02.210459Z"
    }
   },
   "outputs": [],
   "source": [
    "utils.plot_slices(raw_image, overlay=relevance_map_grad_cam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-04T14:41:06.155116Z",
     "start_time": "2018-08-04T14:41:04.904895Z"
    }
   },
   "outputs": [],
   "source": [
    "utils.plot_slices(relevance_map_grad_cam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-04T14:41:07.999565Z",
     "start_time": "2018-08-04T14:41:06.156681Z"
    }
   },
   "outputs": [],
   "source": [
    "get_relevance_per_area(relevance_map_grad_cam)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relevance heatmaps averaged over the dataset\n",
    "Plot an average relevance map of all Alzheimer and all control patients (in the test set). These are the kind of plots that are included in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-04T14:53:15.685789Z",
     "start_time": "2018-08-04T14:53:15.657094Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If None, average over the entire dataset, otherwise pick a number of samples.\n",
    "num_samples = None\n",
    "\n",
    "# The background over which to plot the heatmaps.\n",
    "bg = test_dataset.mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-04T14:47:20.766152Z",
     "start_time": "2018-08-04T14:43:44.768744Z"
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "    avg_relevance_map_AD_backprop,\n",
    "    avg_relevance_map_NC_backprop,\n",
    "    avg_relevance_map_all_backprop,\n",
    ") = interpretation.average_over_dataset(\n",
    "    interpretation.sensitivity_analysis,\n",
    "    net,\n",
    "    test_dataset,\n",
    "    num_samples=num_samples,\n",
    "    seed=0,\n",
    "    show_progress=True,\n",
    "    cuda=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-04T14:53:30.039160Z",
     "start_time": "2018-08-04T14:53:27.705684Z"
    }
   },
   "outputs": [],
   "source": [
    "utils.plot_slices(\n",
    "    bg,\n",
    "    overlay=avg_relevance_map_AD_backprop[0],\n",
    "    overlay_vmax=np.percentile(avg_relevance_map_AD_backprop, 99.9),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-04T14:53:34.245334Z",
     "start_time": "2018-08-04T14:53:33.084959Z"
    }
   },
   "outputs": [],
   "source": [
    "get_relevance_per_area(avg_relevance_map_AD_backprop)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-04T14:53:39.840395Z",
     "start_time": "2018-08-04T14:53:38.111819Z"
    }
   },
   "outputs": [],
   "source": [
    "utils.plot_slices(\n",
    "    bg,\n",
    "    overlay=avg_relevance_map_NC_backprop[0],\n",
    "    overlay_vmax=np.percentile(avg_relevance_map_NC_backprop, 99.9),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-04T14:53:40.996498Z",
     "start_time": "2018-08-04T14:53:39.842037Z"
    }
   },
   "outputs": [],
   "source": [
    "get_relevance_per_area(avg_relevance_map_NC_backprop)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guided Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-04T14:55:46.867186Z",
     "start_time": "2018-08-04T14:54:07.106595Z"
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "    avg_relevance_map_AD_guided,\n",
    "    avg_relevance_map_NC_guided,\n",
    "    avg_relevance_map_all_guided,\n",
    ") = interpretation.average_over_dataset(\n",
    "    interpretation.guided_backprop,\n",
    "    net,\n",
    "    test_dataset,\n",
    "    num_samples=num_samples,\n",
    "    seed=0,\n",
    "    show_progress=True,\n",
    "    cuda=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T02:37:15.234011Z",
     "start_time": "2018-04-11T02:37:11.714954Z"
    }
   },
   "outputs": [],
   "source": [
    "utils.plot_slices(\n",
    "    bg,\n",
    "    overlay=avg_relevance_map_AD_guided[0],\n",
    "    overlay_vmax=np.percentile(avg_relevance_map_AD_guided, 99.9),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T02:24:44.836807Z",
     "start_time": "2018-04-11T02:24:43.688756Z"
    }
   },
   "outputs": [],
   "source": [
    "get_relevance_per_area(avg_relevance_map_AD_guided)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-10T16:28:17.680634Z",
     "start_time": "2018-04-10T16:28:14.680315Z"
    }
   },
   "outputs": [],
   "source": [
    "utils.plot_slices(\n",
    "    bg,\n",
    "    overlay=avg_relevance_map_NC_guided[0],\n",
    "    overlay_vmax=np.percentile(avg_relevance_map_NC_guided, 99.9),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-10T16:28:18.831837Z",
     "start_time": "2018-04-10T16:28:17.682238Z"
    }
   },
   "outputs": [],
   "source": [
    "get_relevance_per_area(avg_relevance_map_NC_guided)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Occlusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-04T15:09:40.182463Z",
     "start_time": "2018-08-04T14:55:49.855852Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(\n",
    "    avg_relevance_map_AD_occlusion,\n",
    "    avg_relevance_map_NC_occlusion,\n",
    "    avg_relevance_map_all_occlusion,\n",
    ") = interpretation.average_over_dataset(\n",
    "    interpretation.occlusion,\n",
    "    net,\n",
    "    test_dataset,\n",
    "    num_samples=num_samples,\n",
    "    show_progress=True,\n",
    "    size=40,\n",
    "    stride=40,\n",
    "    cuda=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-04T15:10:19.549247Z",
     "start_time": "2018-08-04T15:10:17.351648Z"
    }
   },
   "outputs": [],
   "source": [
    "utils.plot_slices(\n",
    "    bg, overlay=avg_relevance_map_AD_occlusion, overlay_cmap=utils.alpha_to_red_cmap\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-04T15:10:24.844273Z",
     "start_time": "2018-08-04T15:10:21.183147Z"
    }
   },
   "outputs": [],
   "source": [
    "get_relevance_per_area(avg_relevance_map_AD_occlusion)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-04T15:10:37.021836Z",
     "start_time": "2018-08-04T15:10:35.530795Z"
    }
   },
   "outputs": [],
   "source": [
    "utils.plot_slices(\n",
    "    bg, overlay=avg_relevance_map_NC_occlusion, overlay_cmap=utils.alpha_to_red_cmap\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-04T15:10:43.735871Z",
     "start_time": "2018-08-04T15:10:40.249085Z"
    }
   },
   "outputs": [],
   "source": [
    "get_relevance_per_area(avg_relevance_map_NC_occlusion)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Area Occlusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-04T15:46:17.602354Z",
     "start_time": "2018-08-04T15:29:58.486970Z"
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "    avg_relevance_map_AD_area_occlusion,\n",
    "    avg_relevance_map_NC_area_occlusion,\n",
    "    avg_relevance_map_all_area_occlusion,\n",
    ") = interpretation.average_over_dataset(\n",
    "    interpretation.area_occlusion,\n",
    "    net,\n",
    "    test_dataset,\n",
    "    num_samples=num_samples,\n",
    "    seed=0,\n",
    "    show_progress=True,\n",
    "    cuda=True,\n",
    "    area_masks=area_masks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-04T17:08:26.880969Z",
     "start_time": "2018-08-04T17:08:24.850133Z"
    }
   },
   "outputs": [],
   "source": [
    "utils.plot_slices(\n",
    "    bg,\n",
    "    overlay=avg_relevance_map_AD_area_occlusion,\n",
    "    overlay_cmap=utils.alpha_to_red_cmap,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-04T17:08:33.736464Z",
     "start_time": "2018-08-04T17:08:32.597485Z"
    }
   },
   "outputs": [],
   "source": [
    "get_relevance_per_area(avg_relevance_map_AD_area_occlusion)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-04T17:08:35.888365Z",
     "start_time": "2018-08-04T17:08:34.337060Z"
    }
   },
   "outputs": [],
   "source": [
    "utils.plot_slices(bg, overlay=avg_relevance_map_NC_area_occlusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-04T17:08:38.521337Z",
     "start_time": "2018-08-04T17:08:37.384719Z"
    }
   },
   "outputs": [],
   "source": [
    "get_relevance_per_area(avg_relevance_map_NC_area_occlusion)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grad-CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T18:01:36.569230Z",
     "start_time": "2018-04-11T17:58:14.810943Z"
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "    avg_relevance_map_AD_grad_cam,\n",
    "    avg_relevance_map_NC_grad_cam,\n",
    "    avg_relevance_map_all_grad_cam,\n",
    ") = average_over_dataset(\n",
    "    grad_cam,\n",
    "    net,\n",
    "    val_dataset,\n",
    "    num_samples=10,\n",
    "    show_progress=True,\n",
    "    resize=True,\n",
    "    interpolation=0,\n",
    "    cuda=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T18:08:36.452545Z",
     "start_time": "2018-04-11T18:08:33.094495Z"
    }
   },
   "outputs": [],
   "source": [
    "utils.plot_slices(mask, overlay=avg_relevance_map_AD_grad_cam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-10T16:33:00.331963Z",
     "start_time": "2018-04-10T16:32:58.379951Z"
    }
   },
   "outputs": [],
   "source": [
    "get_relevance_per_area(avg_relevance_map_AD_grad_cam)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-10T16:33:03.094782Z",
     "start_time": "2018-04-10T16:33:00.333457Z"
    }
   },
   "outputs": [],
   "source": [
    "utils.plot_slices(mask, overlay=avg_relevance_map_NC_grad_cam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-10T16:33:06.053809Z",
     "start_time": "2018-04-10T16:33:04.084773Z"
    }
   },
   "outputs": [],
   "source": [
    "get_relevance_per_area(avg_relevance_map_NC_grad_cam)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save average heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save heatmaps to file.\n",
    "# np.savez_compressed('output/relevance_maps_final_compressed.npz', **{\n",
    "#    'avg_relevance_map_AD_backprop': avg_relevance_map_AD_backprop,\n",
    "#    'avg_relevance_map_AD_guided': avg_relevance_map_AD_guided,\n",
    "#    'avg_relevance_map_AD_occlusion': avg_relevance_map_AD_occlusion,\n",
    "#    'avg_relevance_map_AD_area_occlusion': avg_relevance_map_AD_area_occlusion,\n",
    "#    'avg_relevance_map_AD_grad_cam': avg_relevance_map_AD_grad_cam,\n",
    "\n",
    "#    'avg_relevance_map_NC_backprop': avg_relevance_map_NC_backprop,\n",
    "#    'avg_relevance_map_NC_guided': avg_relevance_map_NC_guided,\n",
    "#    'avg_relevance_map_NC_occlusion': avg_relevance_map_NC_occlusion,\n",
    "#    'avg_relevance_map_NC_area_occlusion': avg_relevance_map_NC_area_occlusion,\n",
    "#    'avg_relevance_map_NC_grad_cam': avg_relevance_map_NC_grad_cam\n",
    "# })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figures and tables for paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T22:47:57.363071Z",
     "start_time": "2018-08-09T22:47:56.827409Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load heatmaps from file.\n",
    "loaded_heatmaps = np.load(\"output/relevance_maps_final_compressed.npz\")\n",
    "avg_relevance_map_AD_backprop = loaded_heatmaps[\"avg_relevance_map_AD_backprop\"]\n",
    "avg_relevance_map_AD_guided = loaded_heatmaps[\"avg_relevance_map_AD_guided\"]\n",
    "avg_relevance_map_AD_occlusion = loaded_heatmaps[\"avg_relevance_map_AD_occlusion\"]\n",
    "avg_relevance_map_AD_area_occlusion = loaded_heatmaps[\n",
    "    \"avg_relevance_map_AD_area_occlusion\"\n",
    "]\n",
    "avg_relevance_map_AD_grad_cam = loaded_heatmaps[\"avg_relevance_map_AD_grad_cam\"]\n",
    "avg_relevance_map_NC_backprop = loaded_heatmaps[\"avg_relevance_map_NC_backprop\"]\n",
    "avg_relevance_map_NC_guided = loaded_heatmaps[\"avg_relevance_map_NC_guided\"]\n",
    "avg_relevance_map_NC_occlusion = loaded_heatmaps[\"avg_relevance_map_NC_occlusion\"]\n",
    "avg_relevance_map_NC_area_occlusion = loaded_heatmaps[\n",
    "    \"avg_relevance_map_NC_area_occlusion\"\n",
    "]\n",
    "avg_relevance_map_NC_grad_cam = loaded_heatmaps[\"avg_relevance_map_NC_grad_cam\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 1 (Average heatmaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T01:48:18.750648Z",
     "start_time": "2018-08-05T01:48:18.704151Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_column(\n",
    "    struct_arr,\n",
    "    axes,\n",
    "    title,\n",
    "    cmap=\"gray\",\n",
    "    vmin=None,\n",
    "    vmax=None,\n",
    "    overlay=None,\n",
    "    overlay_cmap=utils.alpha_to_red_cmap,\n",
    "    overlay_vmin=None,\n",
    "    overlay_vmax=None,\n",
    "):\n",
    "    if vmin is None:\n",
    "        vmin = struct_arr.min()\n",
    "    if vmax is None:\n",
    "        vmax = struct_arr.max()\n",
    "    if overlay_vmin is None and overlay is not None:\n",
    "        overlay_vmin = overlay.min()\n",
    "    if overlay_vmax is None and overlay is not None:\n",
    "        overlay_vmax = overlay.max()\n",
    "    print(vmin, vmax, overlay_vmin, overlay_vmax)\n",
    "\n",
    "    offset = 80\n",
    "    num_slices = len(axes)\n",
    "\n",
    "    intervals = (np.asarray(struct_arr.shape) - 2 * offset) / (num_slices - 1)\n",
    "\n",
    "    axis = 1\n",
    "    axis_label = \"y\"\n",
    "\n",
    "    # for axes_column in zip(titles):\n",
    "\n",
    "    for i, ax in enumerate(axes):\n",
    "        # print(axis_label, 'plotting slice', i_slice)\n",
    "        i_slice = int(np.round(offset + i * intervals[axis]))\n",
    "\n",
    "        plt.sca(ax)\n",
    "        ax.get_xaxis().set_ticks([])\n",
    "        ax.get_yaxis().set_ticks([])\n",
    "        # plt.axis('off')\n",
    "        plt.imshow(\n",
    "            sp.ndimage.rotate(np.take(struct_arr, i_slice, axis=axis), 90),\n",
    "            vmin=vmin,\n",
    "            vmax=vmax,\n",
    "            cmap=cmap,\n",
    "            interpolation=None,\n",
    "        )\n",
    "        plt.text(\n",
    "            0.03,\n",
    "            0.97,\n",
    "            \"{}\".format(i_slice),\n",
    "            color=\"white\",\n",
    "            horizontalalignment=\"left\",\n",
    "            verticalalignment=\"top\",\n",
    "            transform=ax.transAxes,\n",
    "        )\n",
    "\n",
    "        if overlay is not None:\n",
    "            plt.imshow(\n",
    "                sp.ndimage.rotate(np.take(overlay, i_slice, axis=axis), 90),\n",
    "                cmap=overlay_cmap,\n",
    "                vmin=overlay_vmin,\n",
    "                vmax=overlay_vmax,\n",
    "                interpolation=None,\n",
    "            )\n",
    "\n",
    "        if i == 0:\n",
    "            plt.title(title + \"\\n\")\n",
    "            # plt.ylabel('Backpropagation', rotation=0, size='large')\n",
    "            # ax.get_yaxis().set_label_coords(-0.5, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T01:48:20.463446Z",
     "start_time": "2018-08-05T01:48:18.752433Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 4, figsize=(11, 7.5))\n",
    "axes = axes.T\n",
    "\n",
    "# bg = mask\n",
    "bg = test_dataset.mean\n",
    "vmax = 550  # make the background brain a bit lighter\n",
    "\n",
    "plot_column(\n",
    "    bg,\n",
    "    axes[0],\n",
    "    \"Sensitivity Analysis\\n(Backpropagation)\",\n",
    "    overlay=avg_relevance_map_NC_backprop[0],\n",
    "    overlay_vmax=np.percentile(avg_relevance_map_NC_backprop, 99.9),\n",
    "    vmax=vmax,\n",
    ")\n",
    "plot_column(\n",
    "    bg,\n",
    "    axes[1],\n",
    "    \"Guided\\nBackpropagation\",\n",
    "    overlay=avg_relevance_map_NC_guided[0],\n",
    "    overlay_vmax=np.percentile(avg_relevance_map_NC_guided, 99.9),\n",
    "    vmax=vmax,\n",
    ")\n",
    "plot_column(\n",
    "    bg, axes[2], \"Occlusion\\n\", overlay=avg_relevance_map_NC_occlusion, vmax=vmax\n",
    ")\n",
    "plot_column(\n",
    "    bg,\n",
    "    axes[3],\n",
    "    \"Brain Area\\nOcclusion\",\n",
    "    overlay=avg_relevance_map_NC_area_occlusion,\n",
    "    vmax=vmax,\n",
    ")\n",
    "# plot_column(bg, axes[4], 'Grad-CAM\\n', overlay=avg_relevance_map_AD_grad_cam, vmax=vmax)\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.savefig('heatmaps-nc.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 1 (Most relevant brain areas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T01:48:45.290050Z",
     "start_time": "2018-08-05T01:48:36.196707Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "most_relevant_areas_per_method = [\n",
    "    get_relevance_per_area(avg_relevance_map_AD_backprop)[:4],\n",
    "    get_relevance_per_area(avg_relevance_map_AD_guided)[:4],\n",
    "    get_relevance_per_area(avg_relevance_map_AD_occlusion)[:4],\n",
    "    get_relevance_per_area(avg_relevance_map_AD_area_occlusion)[:4]  # ,\n",
    "    # get_relevance_per_area(avg_relevance_map_AD_grad_cam)[:4]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T01:48:45.319305Z",
     "start_time": "2018-08-05T01:48:45.291967Z"
    }
   },
   "outputs": [],
   "source": [
    "lines = []\n",
    "for i in range(len(most_relevant_areas_per_method[0])):\n",
    "    line = \" & \".join(\n",
    "        [\n",
    "            \"{} ({:.1f} \\\\%)\".format(areas[i][0].replace(\"_\", \"\"), areas[i][1] * 100)\n",
    "            for areas in most_relevant_areas_per_method\n",
    "        ]\n",
    "    )\n",
    "    lines.append(line)\n",
    "print(\" \\\\\\\\ \\n\".join(lines))  # output in latex format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 2 (Euclidean distances between heatmaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T01:48:48.987166Z",
     "start_time": "2018-08-05T01:48:48.956253Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "relevance_maps_AD = [\n",
    "    avg_relevance_map_AD_backprop,\n",
    "    avg_relevance_map_AD_guided,\n",
    "    avg_relevance_map_AD_occlusion,\n",
    "    avg_relevance_map_AD_area_occlusion,\n",
    "]\n",
    "relevance_maps_NC = [\n",
    "    avg_relevance_map_NC_backprop,\n",
    "    avg_relevance_map_NC_guided,\n",
    "    avg_relevance_map_NC_occlusion,\n",
    "    avg_relevance_map_NC_area_occlusion,\n",
    "]\n",
    "names = [\"backprop \", \"guided   \", \"occlusion\", \"area occl\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T01:49:12.392770Z",
     "start_time": "2018-08-05T01:49:08.993948Z"
    }
   },
   "outputs": [],
   "source": [
    "scale_factor = 1e-4\n",
    "\n",
    "print(\n",
    "    \"Euclidean distance between average heatmaps for AD / NC samples in {}\".format(\n",
    "        scale_factor\n",
    "    )\n",
    ")\n",
    "print()\n",
    "\n",
    "print(\"\\t\\t\" + \"\\t\".join(names))\n",
    "\n",
    "for a_AD, a_NC, a_name in zip(relevance_maps_AD, relevance_maps_NC, names):\n",
    "    print(a_name, end=\"\\t\")\n",
    "    for b_AD, b_NC, b_name in zip(relevance_maps_AD, relevance_maps_NC, names):\n",
    "        print(\n",
    "            \"{:.2f} / {:.2f}\".format(\n",
    "                interpretation.heatmap_distance(a_AD, b_AD) / scale_factor,\n",
    "                interpretation.heatmap_distance(a_NC, b_NC) / scale_factor,\n",
    "            ),\n",
    "            end=\"\\t\",\n",
    "        )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T01:49:17.939098Z",
     "start_time": "2018-08-05T01:49:17.497766Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Euclidean distance between AD and NC heatmaps for each method in {}\".format(\n",
    "        scale_factor\n",
    "    )\n",
    ")\n",
    "print()\n",
    "\n",
    "print(\"\\t\".join(names))\n",
    "for a, b in zip(relevance_maps_AD, relevance_maps_NC):\n",
    "    print(\n",
    "        \"{:.2f}\".format(interpretation.heatmap_distance(a, b) / scale_factor),\n",
    "        end=\"\\t\\t\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export to Animation and MRIcron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T01:53:32.841720Z",
     "start_time": "2018-08-05T01:53:32.636364Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "anim = utils.animate_slices(\n",
    "    test_dataset.mean,\n",
    "    overlay=avg_relevance_map_AD_guided[0],\n",
    "    overlay_vmax=np.percentile(avg_relevance_map_AD_guided, 99.9),\n",
    "    axis=2,\n",
    "    reverse_direction=True,\n",
    "    interval=70,\n",
    ")\n",
    "plt.close()  # suppress plot output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T01:53:32.987451Z",
     "start_time": "2018-08-05T01:53:32.953878Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\n",
    "    \"animation.ffmpeg_path\"\n",
    "] = \"/home/johannesr/ffmpeg-3.4.1-64bit-static/ffmpeg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T01:53:38.282946Z",
     "start_time": "2018-08-05T01:53:34.184483Z"
    }
   },
   "outputs": [],
   "source": [
    "# Display the animation inline.\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T01:54:39.890200Z",
     "start_time": "2018-08-05T01:54:39.045394Z"
    }
   },
   "outputs": [],
   "source": [
    "anim.save(\"data/anim-guided.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You can also save the relevance map in NIFTI format and load it as an overlay in MRIcron (https://www.nitrc.org/projects/mricron) later.\n",
    "# utils.save_nifti('guided_backprop.nii', relevance_map_guided[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
